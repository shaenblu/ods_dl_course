{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for W1\n",
      "Gradient check passed!\n",
      "Checking gradient for B2\n",
      "Gradient check passed!\n",
      "Checking gradient for B1\n",
      "Gradient check passed!\n",
      "Checking gradient for W2\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302491, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302451, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302476, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302536, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302488, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302381, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302578, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302424, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302486, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302473, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302535, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302558, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302561, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302531, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302597, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302473, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302530, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302513, Train accuracy: 0.148222, val accuracy: 0.140000\n",
      "Loss: 2.302480, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302503, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb1e629ba8>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3X1wZfdZ2PHvc99fpHsl3atdab12bINps4BJwtpAQgwBGmzK2ISxwW6YxsCMS8HTMi20nqE11ExnCimUoXXbmDbDy5A4gfLiMpux3ZBCW5LgjWM72TiO146zq6y0u7qS7pXui+7b0z/Oudq7srQ60n09R89nRqP7cu45v717zqPf7zm/F1FVjDHGHA6hURfAGGPM8FjQN8aYQ8SCvjHGHCIW9I0x5hCxoG+MMYeIBX1jjDlELOgbY8whYkHfGGMOEQv6xhhziERGXYDt8vm83njjjaMuhjHG+MrnPve5ZVWd3Wu7sQv6N954I6dPnx51MYwxxldE5GtetrP0jjHGHCIW9I0x5hCxoG+MMYeIBX1jjDlELOgbY8whYkHfGGMOEQv6xhhziIxdP/0Dq5fh//5Wb/sIReDbH4TJo30p0r698BFY+epojm1MENzyXrj+ttEc+9xn4ez/6m0fmWNw8if7U55dBCfoN6rw1x/sYQfuWsHRJLzrn/SlSPuyuQF/9o/dJzL84xvjewrnPg0P/sVoDv/JfwNf+3/0dP0eP2lB37N0Hn5l7eCfV4V/Owfly/0r0350jnvPf4a3v380ZTDGz558PxReG93xNy7BN78P7vvd0ZXBA8vpd4hAKgeVwmiO3zluKjea4xvjd6kcVJZHd/zKsi+uXwv63VI5KI/opOkcN50fzfGN8bt0Hior0G4P/9itJlRXITX+168F/W7pvNX0jfGrVB60BbUe0rwHVV11fvug0mZBv1sqP7rmYcVq+sb0pHPtjKLi1rl+fVBps6DfLZWD8ohq+uVlCMcgNjGa4xvjd6kZ5/coUrRlC/r+lM5BfR2am8M/dqXgtDTEumsacyCdfPooWus+aqlb0O+WGmXzsOD80THGHMxI0zude3IW9P2lc9KMqnnogxPGmLHVSa2M5PrtBP2Z4R97nyzod+ucNKNqHvogH2jM2IomIZoe3Y3cRBbC0eEfe58s6Hfr1LRHcTO3XPBFPtCYsZYe0VgbH7XULeh3G1VOsLnp3ED2yUljzNhKjWisTcU/lTYL+t0SUyCh4ad3Oiep3cg1pjfpEY216fS+8wEL+t1CIUjODL956KM+vsaMtVGNtSkv++ImLngM+iJyp4i8IiJnReSRHd7/ZyLyJRF5SUQ+KSJv6XrvAyLyqvvzgX4WfiBGUVPYGs3nj5qCMWOrM+ma6vCOqRqs9I6IhIHHgbuAE8ADInJi22afB06q6q3AHwO/7n52Bvhl4DuA24FfFpHp/hV/AFLupE3D1DmeT04aY8ZWOg/NGjQqwzvmZgnaDd9U2rzU9G8Hzqrq66paB54E7uneQFU/paqdb/kzwHH38Q8Cz6rqiqquAs8Cd/an6AMyirv/ZavpG9MXqRGMtfHZDLlegv51wPmu5wvua7v5aeAT+/msiDwkIqdF5PTlyyNaxKRjFHNyV5YBgeTUcI9rTNCMYqyNz2bI9RL0d5oMZseEmYj8BHAS6Kxb6OmzqvqEqp5U1ZOzs7MeijRAnfROuzW8Y3ZuAoXCwzumMUGUHsFYG591xPAS9BeA67ueHwcubN9IRH4A+CXgblXd3M9nx0o6DyhUhzgnt4+6exkz1rZq+kMM+ltdrv1xDXsJ+s8Bt4jITSISA+4HnureQETeDnwIJ+Bf6nrraeC9IjLt3sB9r/va+BpV89AnJ4wxY21rgOUwr19/3ZPbM+irahN4GCdYvwx8XFXPiMhjInK3u9kHgQngj0TkBRF5yv3sCvCrOH84ngMec18bX6OYtMlHfXyNGWvxDISiw79+I0mIpYZ3zB5EvGykqqeAU9tee7Tr8Q9c47MfBj580AIO3ahqCql3Du94xgSVyPA7Y/ispW4jcrcb9pz67ZazvqaPThpjxlp6yGNtKgXf3MQFC/pvtrXk2pCCfnUNtO2bfKAxYy815LE2ZX9Ni25Bf7tI3MkLDqt56KMFlY3xhaGnd5Z91VL3lNP3g/Jmk//w7Fd62kc4LDz4zhuZH2ZNoWuGzT/49Bt8rTDE4ePGBMz3v/Uo35Ue8vTKZafL9d+cXeYvv3xp7+2v4dhUkp/67pv6VLCdBSbobzbbfPRvz/W0j3K9RSYR5eeGedK4f1zWQ1P86z8/QywcIhq2xdGN2a9qo8WLC2v80d/NQ60IrcbgV7JqVKFRhnSODz7zCi8tFElEDp5AufX4lAV9r2bSMc481tu0Pm977BkWi1WneVj6ep9Ktge3GXqxlQbgN3/82/jhW48N59jGBMjPP/l5Tn9t9cq6FJUCTM4N9qBdUzAsrtV439uv49/f922DPWaPLKffZS6TYKlYc26qDutGrnuchbrTx3c+mxjOcY0JmLlskoulGu3kEMfauMdoJXJcWq/54vq1oN9lPptgsVhzagrDmpO7UoDYJIsbzrHmssnBH9OYAJrPJmi0lFIo67wwjBSt21JflQxthTkL+v4yP5V0gn4qD6061DcGf9DKMqRzLK5VCQkcmYwP/pjGBFCnln2xNeG8MIwePO54gKWmc8xjPqi0WdDvMp9JsFKu04i767wMq3mYyrNYrDE7GScatv8SYw5i3g24F+rO/bGhpGjdGHHBTc9aTd9nOv9hq2ScF4bVPEzlWCrVLLVjTA861+/5qttaHkpNfxkkzLmK00vIcvo+c2zKCboX25POC8Oo6VdWIO3U9I/54IQxZlzl0jFi4RAX1puQnB5Opa3cqbTVSUbDZJMD7iLaBxb0u3RqCp2m2sBPGlUoL6MpJ6fvh6ahMeMqFBKOZuNut+v8kCptha1K23w2gcj4j7GxoN9lLuME3XM1N80y6OZhfQNam2zGpynXW75oGhozzuYzbmeMYQ2wdCdbWyz6p9JmQb9LOh4hk4hwfiME4fjgawru/tfcewiW0zemN3PZzlibIU2l0knvFGsW9P3q2FSSxdLmcGoKbnevy+49BMvpG9Ob+Skn6GsqN7SOGO1Unovrm77orgkW9N9kLpu4MhXDwIO+UxNZbKS3jm2MObj5TIJ6q0016t7IbbcHd7BWE6prVCJTtNrqm+vXgv4288NsHrr7X9hMIQJHJv1x0hgzrjop0qJkQFtQWxvcwaqrgLKK01L3yz05C/rbzGWSLG/UaSWHMCe3u/83KknyE3FiPczOZ4y5EniX1e12PcjWunv9LrvpWavp+9T8lPMfV45MDX5EX6UA4RhfXRfL5xvTB53r92LTHZU7yKDvttQXfTQFA1jQf5NOTaEoWaivQ3NzcAdzF19YKm36ppZgzDjLp+NEQsLC1lQMA2ytu39QFjaTxCMhplLjPzALPAZ9EblTRF4RkbMi8sgO798hIs+LSFNE7t323q+JyBfdnx/vV8EHpRP0C9qZtGnAzcO0091r3ie1BGPGWSgkHM0k+Fq1M8BykEHfTc9Wk74ZmAUegr6IhIHHgbuAE8ADInJi22bngAeBj2z77N8H3gG8DfgO4BdFJNN7sQencyNoa6a+QdYUyss0EzOsbzatpm9MnxybSnC2HHOeDPT6dSqEr23EfXX9eqnp3w6cVdXXVbUOPAnc072Bqr6hqi8B2/tHnQD+SlWbqloGXgR6W95qwCbiESYTERY2h1FTKFCNTAH+ufNvzLibyyY5V1KIprfGwgxEZRkSWRZKTd/k88Fb0L8OON/1fMF9zYsXgbtEJCUieeA9wPXbNxKRh0TktIicvnz5ssddD858trt5OMiTpkAp1An6/jlpjBlnncWQND3gHniVAprKc7Hkn9G44C3o75So8rSklKo+A5wC/gb4KPBpoLnDdk+o6klVPTk7O+tl1wM1l03yWsWdnnVQzcPmJmyWWHGnYLCavjH9MZdJUG+2aSUGPNamvEwzPk2zrb66fr0E/QWurp0fBy54PYCq/ltVfZuq/j2cPyCv7q+IwzefSfBqKQoSGlxNwb1BfNldEP1IxlbMMqYfjrndNqvRqYHX9CtRZ8ElP82b5SXoPwfcIiI3iUgMuB94ysvORSQsIjn38a3ArcAzBy3ssMxPJbhcbqDJmcHVFNygf6GRJj8RJx4JD+Y4xhwynQC8HsoONj1bXt5ajzdQNX1VbQIPA08DLwMfV9UzIvKYiNwNICK3icgCcB/wIRE54348CvwfEfkS8ATwE+7+xtp8NoEqNBMzg+uy6f4xOVdL+eqEMWbcda6nVckMrtKmCpXC1ip7frqGI142UtVTOLn57tce7Xr8HE7aZ/vnajg9eHylU1OoRqeIDirou/t9vZJk7oh/Thhjxl1+whmgtdyehGYV6mWIpft7kM0StBtcbqWJRULMpGP93f8A2YjcHXSmRNgITw2upuDu99WNmE3BYEwfhd0BWkuNAY7K7SyI3pjw1cAssKC/o6sWSB/gjVxFOF9L+OomkDF+MJdNcH5zgMueuvs8t5ncWnHPLyzo72AyEWUiHnGah9VVaLf6f5DKMu3ENG1CvsoHGuMHc9kEX612lj0dXNB/vZL03fVrQX8Xc1m3eahtqA5gTu7yMpuxma1jGWP651g2wasbAxxr4+7TmYLBXy11C/q7mM8mOF8f4FQMlQLliNPdy09DuI3xg7ls8kpOfyDXr7PPi62JrXEBfmFBfxfz2QRfrbj/mYOoKVQKzuo+2MAsY/ptPpugRAoNRQeT3ikv0w4nqJKwnH5QzGWTfLXq/mcO6KQpaIZcOkYiagOzjOknJ2Uq1GPTA6q0rbAZc0bj+m3eLAv6u5jPJii03Vmg+908bLehusLF1oTl840ZgE7KtBKdGtCN3GUq7gy5fruGLejvYj6b2FrwuO/LJtbWQNt8vZ7yXS3BGD+YnYwTDgmlQY3KLS9TDGWJhUPkfDQwCyzo72o+m6ROlEZkov81ffck/FrVf929jPGDcEg4Mhl3ZrEdUE2/oJMczcYJhfwzMAss6O+q02SrRgbQPHT/iCzU075rGhrjF3PZBJdaA6i0AVRWuNSaYD7jv5a6Bf1dZBIRUrEwpXC2/81Dd38rOmk1fWMG5Fg2yYVGGmpFaDX6t+NGDeobfL2e8mWlzYL+LkSE+WyCFZ3sf03BbTk4Qd9/NQVj/GAum+B8rTMqt49TLLvx4Fw1xbzP+uiDBf1rms8mudSe7P+NXPekWSFjNX1jBmQ+m2CpOeE86WfFzW2pX2pPMO+zPvpgQf+a5rIJluppp2aunlaI9KZcoB5OUyfqy+ahMX4wn01uLUfa1xRtV0vdb1MwgAX9azqWTXC+noTWJtQ3+rfjyjLr4SwzNjDLmIGZyyYo6ADG2nSCPhnfTcEAFvSvaS6bvHLS9LmmUCTju+HbxvjJfDbBqrpjbfqZ03djQUEzvmypW9C/hvmragp9zOuXl7lsPXeMGagjk3GK4ub0+1ppW6ZNmEooTT7tv3mzLOhfw9xVNYU+Bv1KgaWG9dE3ZpAi4RC5yTTlcJ8XQ6oUKIczHMmkfDcwCyzoX9OxbJLC1lQMfTppVNHyMovNCY5N+e8mkDF+MpdNOLPZ9rOmX15mTfyZzwePQV9E7hSRV0TkrIg8ssP7d4jI8yLSFJF7t7336yJyRkReFpHfFh8tJplJRqhEnJn0+lZTqJeR1iarOmk5fWMGzEnRTva9pb7c9mfPHfAQ9EUkDDwO3AWcAB4QkRPbNjsHPAh8ZNtn3wm8C7gV+BbgNuB7ei71kIgI2cwUDYn2r6aw1UffcvrGDNpcNsHF5gTax6Dfaan79fr1UtO/HTirqq+rah14ErinewNVfUNVXwLa2z6rQAKIAXEgClzsudRDNDeVdGbq69fdf3egl1/v/BvjJ8eySS61JtA+pne0UqDQnvBtS91L0L8OON/1fMF9bU+q+mngU8Ci+/O0qr6830KO0nyn22a/0jvuflZtCgZjBm7OnSJdKgVnHYtetVtIddW3ffTBW9DfKQfvaXiqiHwj8FbgOM4fiu8TkTt22O4hETktIqcvX77sZddDM59NcLGfNQW3mdlIzJCM2cAsYwbJmT8rg2gLNou977CygqAUfDoaF7wF/QXg+q7nx4ELHvf/PuAzqrqhqhvAJ4Dv3L6Rqj6hqidV9eTs7KzHXQ/HnHsjqLXRpz9G7h+PWOZIf/ZnjNnV/FT3AMs+5PU79+TUv/NmeQn6zwG3iMhNIhID7gee8rj/c8D3iEhERKI4N3F9lt5x+upLv3L6lWUaRMhmp/uzP2PMro5Mxq+sgNePFK3bUi9KhvyE/wZmgYegr6pN4GHgaZyA/XFVPSMij4nI3QAicpuILAD3AR8SkTPux/8YeA34AvAi8KKq/s8B/DsGppPTDzfWobnZ+w7LBVbJMD+d6n1fxphrioZDaCrnPOlHitbdh6TzhH04MAsg4mUjVT0FnNr22qNdj5/DSfts/1wL+Ec9lnGk5rOJKzP1VQqQOdbT/lrlZZbbk76cktUYP4pOzsIq/emr77YWoj5Oz9qI3D1MpaKUQlnnSR9qCs31y+5NIAv6xgxDamrOedCP9I57XyA1bUE/sESEUNptHvahpqDlZXfxFH/e+TfGb3LTWSoa78uNXK0sU9IUR7MTfSjZaFjQ9yA66f5V70PQD1cLzjKJPu3ja4zfOCnaSRrrl3reV6PktNTnfTxvlgV9D1JTbtDvNb3TrBNtbjgr7lhO35ih6CymUi/13u26vn7Z98ucWtD3IDMzS0uFdrnHk8ZtKVSiU6Tjnu6hG2N6dGwqyYpO0u7DPTktX2bF51OoWND3YG56gjUmqBV7DfruSZfK914oY4wncxmnB16o2o/07CorOskxH9+Ts6DvwXzGGcpdL/WYE3RrGpFJC/rGDMvRTIIVnSS22eMAS1Xi9RVWJcPspD8HZoEFfU/m3BtB7V6nYnDTO4msf7t7GeM3sUiIWmyaaHsT6pWD72izRFib1GPTvh2YBRb0PemslRuq9lZTaLh/NNLT8/0oljHGq86o3F766rstdfV5etaCvgcz6RhFyRDdXO1pP5XVi7RVmM5bTd+YYQql3Ykce7mZ686/FfV5etaCvgciQj0+TbJZ7GlO7traRdZIMzeV7mPpjDF7iWfcoN/DWBt1e+/FfJ6etaDvUTuZI0Qbqgev7Tc3ln09JasxfpWecaZiqBUP3hmjuuZ8dmJ6ri9lGhUL+h6F026TroecoFQKFMj4dvEFY/wqk3MC9cbK0oH3sbHqfDabP9qXMo2KBX2P4m6Trr1x8KAfqa1QCmWYsIFZxgxVPjdLQ8NbtfWDqBUvUdMoR2ZyfSzZ8FnQ9yg17fx1X189eE0hUV+lHrPFU4wZtvmpFKs9zr/TXL9MgYyv590BC/qeZXJON8v1wgGDfrtNul2imfB3LcEYPzqajVPQSbSXSRPLzmSJfh6YBRb0PcvNOkG/unbxYDuorRGmjfi8j68xfhSPhFkPZwn3MNYmslmgHM4SDfs7bPq79EM0l8uyrkka6wfL6XemcIhmxmvhd2MOi1p0mkT94EE/UV+jFpvpY4lGw4K+RzOpGKtMbvXV3a/V5UUAklP+7uNrjF814zOkW8UDf36itUYrYUH/0AiFhPXw1IGbhyX3XsDkjL/7+BrjW+k8k7oBrca+P6qNKilqkPZ/etaC/j7UolPE6wcbnFVZde4FzMzavDvGjELYnT6hcoABWhvu9Rub9H961lPQF5E7ReQVETkrIo/s8P4dIvK8iDRF5N6u198jIi90/dRE5Ef6+Q8YpmZihlRr7UCf3Sw6J03+6LF+FskY41Ei63S7Lly6sO/PFi456dlEANKzewZ9EQkDjwN3ASeAB0TkxLbNzgEPAh/pflFVP6Wqb1PVtwHfB1SAZ/pQ7pHQVI6pdol2a//z77TKBcqaYHJicgAlM8bsZXLGCfrF5f13uy4uX3D34f/0rJea/u3AWVV9XVXrwJPAPd0bqOobqvoScK1oeC/wCVXtYULr0QpPzBKXBqvF/ad4QpUCpVB2AKUyxngxlXdSq51UzX500rNTAUjPegn61wHnu54vuK/t1/3ARw/wubHRWfxk+eLX9/3Z2OYKlehUv4tkjPFoZtZJrXZSrfux6Xa5zs36Pz3rJejvtESM7ucgIjIPfCvw9C7vPyQip0Xk9OXLva9YPygTbtNu9QDNw2RjlXrc/929jPGrhDtGprm+/xjT2limSYho2v/XsJegvwBc3/X8OLDfOyE/Bvypqu7YV0pVn1DVk6p6cnZ2fO+OZ/NO0C/vc/6dRqvNpJZoJ/1/whjjW+EI6zKxtRjKfkilwIZkIOT/Do9e/gXPAbeIyE0iEsNJ0zy1z+M8gM9TOwBTOadpV9vnTH2XSjVylK6s3mOMGYlyZIpIbf9BP7a5QjkSjPTsnkFfVZvAwzipmZeBj6vqGRF5TETuBhCR20RkAbgP+JCInOl8XkRuxGkp/FX/iz9cobQzWVpzn9MrXyoUSEiDmE3BYMxIbcamSTb2H/STjSKNeDBmyPU0sbuqngJObXvt0a7Hz+GkfXb67Bsc7Mbv+IlP0iCy73U2O318Uz5fcccYv2slcmTWX6Nab5GMhT19Zr3WIKtFWslghDH/J6iGSYSNyBTRzf3VFNZXnN4CmZy/V9wxxu8knWNG1lkq1Tx/ZqlYY0ZKhCb8PwUDWNDft83YDIn6KqreOzB1pmNOTVnQN2aUopkjTLPO4pr34UKLq2WmKBPL+H80LljQ37dWYpopSqxWvE/a1FmtRwIwWZMxfpacOkpUWhSWvXfGWF2+SEiUVACmYAAL+vsm6TwzrHNhrer5M1vr6toCKsaMVGcaheI+VsArrixe9Vm/s6C/T9HJI8xIiaWi95xguLpCUyIQt3l3jBmlziyZ5X2sgFd1p2CIBGCGTbCgv2/JqSNkpMrFtZKn7ZutNonGKtXoNMhOg5uNMUPjdrtu7GN65XpnBG9AWuoW9PepczPWa/Pw8sYm05Ro2hQMxoyeG7hb+xhroxvuYuqp3CBKNHQW9Pep022rsuKteXhhrUZOSmhAThhjfM29DkPVguePhGqde3LBuIYt6O+XW1PozLq3l6VijWnWCQekj68xvhZL0QglSDTWqDVae26+sdkk3SyyGZ6ASGwIBRw8C/r75Xa7bHtsHi4Wq+RkfWvVHmPMaNXjM8xIiYseBmgtFZ2WeiMAC6J3WNDfr04Tr1LwNEDr4uo6GanYvDvGjIl2MkeOdS6s7R30F4tVpllHk8FI7YAF/f1LTqMIGS2y5mGAVtnt7mUDs4wZD+GJWafbdWnvsTaLxRo5WScckO6aYEF//0Jh6rEpcpRY9NBXv9rpGhaQm0DG+F0sk2dG1j1dv515d+IBaqlb0D+AdqozadPeNYWtVXqspm/MWIhMHiEn654GWC6uOffkwhMW9A+1cNqpKeyVE2y19UrXsIAM7DDG91I5kmyyvLK256arawWiNAPVUregfwDRyVlm2Hsqhsvrm0ypO3LXavrGjAf3Wqx6WCB9s7NKXoCuXwv6ByDpPPnQ3jnBxWKVGVlHEUgGY9UdY3zPrbU3PCyQ3gjYFAxgQf9gUjmybHCxWL7mZkvFGjOUaMWnIORtlR5jzIC5ATxcXWGzufsArUq9SbzuLphk6Z1DLp0nTJuNtWsP0Lrg3vm37prGjBH3epyhxMXi5q6bLRZrzMi6+xkL+oebW1Ooly5fc4DWUrFKPrQemGXWjAkEt9Y+IyUWi7v3wHNa6m7Qt/TOIef+1U831yhVm7tutliscSRURgLUNDTG9xJZNBTZc63cRbel3g4nIJYeYgEHy1PQF5E7ReQVETkrIo/s8P4dIvK8iDRF5N5t790gIs+IyMsi8iURubE/RR+h7prCNfrqdwZ2BOnOvzG+J4Imc8xw7c4YS+68WZKaCdRaGHsGfREJA48DdwEngAdE5MS2zc4BDwIf2WEXvw98UFXfCtwOeF+9YFy5Tb0ZWWfxGn31l9YqTLTXA9U0NCYIQuk8RyPrLF5j2dMLxRpHwxuBuyfnpaZ/O3BWVV9X1TrwJHBP9waq+oaqvgS0u193/zhEVPVZd7sNVfW+DP242roRtHtNodVWqusrhGlZTd+YcZPOcSRc3qOmX+NIeCNw16+XoH8dcL7r+YL7mhffBKyJyJ+IyOdF5INuy8HfInE0NkleSiztciOosLFJVovOE8vpGzNeUjlnKgYPOf2gtdS9BP2dkll7zynsiADvBn4BuA24GScNdPUBRB4SkdMicvry5b0HTIwDSc1wLLZ7TWHR7aMPWNA3Ztyk8mS1uGdOP9MuBu769RL0F4Dru54fBy543P8C8Hk3NdQE/gx4x/aNVPUJVT2pqidnZ30ysVE6f83mYWfxlM62xpgxks6Taq2ztlGm3my/6e1qvUWlUibergaqjz54C/rPAbeIyE0iEgPuB57yuP/ngGkR6UTy7wO+tP9ijqFUnlxo936+i8Ua0xK8Pr7GBIJbe89qeccVtJZKzjKnzrbBun73DPpuDf1h4GngZeDjqnpGRB4TkbsBROQ2EVkA7gM+JCJn3M+2cFI7nxSRL+Ckin5nMP+UIUvnybadOfV3GqC1VKxxJNQ5aYJVUzDG97q6Xe+U1w9ySz3iZSNVPQWc2vbao12Pn8NJ++z02WeBW3so43hKzZBurVGpNynVmmST0avevlCs8e54BUITEE2MqJDGmB25gTwnJS7s0G1zcc29iQuBq7TZiNyDSuWJtOuk2NxxiuWlYpX5yEbgThhjAiF1pdv1jtfvYU7vmF10+urvMn/HYrFGPoB9fI0JBPe6nI/u3BljsVjleLx81bZBYUH/oNy//rkdFlNpt5WLpRrTlKymb8w4cte3uD5R2aWlXuP6eBUkBImpYZduoDzl9M0O3GCeC61zYdtJs1zepNFSJlvFwDUNjQmEcBQSUxyLlHdsqV9Yqznp2dAMhIJVNw7Wv2aY3L67b0lU3zQq16k5KIn6auD6+BoTGOk8R0IbO6Z3lkrBTc9a0D8otwZ/Q7zyppNmsVgjySbh9qbV9I0ZV6mBi9mEAAAONElEQVQ801Li8sYmjdaVAVq1RouVcp1pDd4UDGBB/+DikxCOcSz25pzgUrF2pY+v5fSNGU+pHJl2EVW4tH5lBa3OYK3JdjGQLXUL+gclAqkcR8JvnmnzQrHK0fCG8ySAzUNjAiGdI9lYA7hqiuUL7nTpycZqICttFvR7kcozzTobm03Wa42tl5eKNW5OV7e2McaMoVSeWH0V0KsqbkulKiHaRDbXAnn9WtDvRdppHgJXpXgWizXekqhtbWOMGUPpPNJukqHyput3ig0EDWRL3YJ+L1J5Uk23edhdUyjWOB5zB3YEsHloTCC41+bxbZ0xloo13pKoXLVNkFjQ70UqR2xzBWCrr2+7rSwVaxyNbEAoCvHMKEtojNmNm7r5xnTtqr76F9ZqfOOE+0fAgr65SjpPqL5OTJpbNYWVSp16q+303knnA7WgsjGB4qZeb0pV35TTv5KetfSO6ebWAr4hfWXStc7vKYLZx9eYwHCvz+u2dbteKta4Ll65apsgsaDfC7cWcMtEbaum0Pmdbq5BamZkRTPG7MGttM1Hy1xar9FstdlstljeqDMX2bhqmyCxoN8L94S4KVHdygl2fsfrq4FsGhoTGLEURFPMhtZpuwO0LhadQVp5KTn34yKxERey/2zCtV64Tb/jiSqLS1dq+tGwEK4WAtk0NCZQUnmm1FksZbHo1PYBsroeyFo+WNDvTdec3Ou1JhubTZaKNY5NRpBayWr6xoy7dI6J1pWxNs22E/QnWmuBvX4t6PciOQ0Is+5auEvFKhfWqnzTZB1qWE7fmHGXypHYWAac1Gyj5ax3Ha+vwtT1oyzZwFhOvxehMCSnmVKnprBYrLFUqnFzqtPHN5g1BWMCI5UnXC2QioWd67dYZTIRcdOzlt4xO0nnmWy7OcE1pxfPW45Xtt4zxoyxdB6pFJjLJlgq1mi02sxn4lAuBHYKFU81fRG5U0ReEZGzIvLIDu/fISLPi0hTRO7d9l5LRF5wf57qV8HHRirvLJYCfGmxRL3Z5lgsuH18jQmUVA4aFW6cFBaLVZZKNW7KKLTqgb1+9wz6IhIGHgfuAk4AD4jIiW2bnQMeBD6ywy6qqvo29+fuHss7flIzhKoF8hMxPn/OCf5HwjaXvjG+4F6jN6c3WSzWuLBW4xtSwZ2CAbzV9G8Hzqrq66paB54E7uneQFXfUNWXgPZOOwi0dB7Ky8xlE5y54KR5ZmQdELuRa8y4c1Owb0lWuFiqsbyxyQ2JYKdnvQT964DzXc8X3Ne8SojIaRH5jIj8yL5K5wepPFRXmJ+M02w7d/4z7aLTsycUHnHhjDHXtDUVQxn38g18etbLjdydZgzTfRzjBlW9ICI3A38pIl9Q1deuOoDIQ8BDADfccMM+dj0G0nnQNjdPOouoREJCohHcPr7GBIp7nc5FKsAUQNeqd4c3vbMAdHdYPQ5c8HoAVb3g/n4d+N/A23fY5glVPamqJ2dnZ73uejy4eb/O/NtHMwlCleB29zImUNwUbE5KWy9tPQ7oNewl6D8H3CIiN4lIDLgf8NQLR0SmRSTuPs4D7wK+dNDCjqWuhRgA5rIJKC8H9oQxJlASUxCKkHXH2oCbng3HITYxwoINzp5BX1WbwMPA08DLwMdV9YyIPCYidwOIyG0isgDcB3xIRM64H38rcFpEXgQ+Bfw7VQ1W0Hebh0fDzkpZc9kEVAqW3jHGD0QglSNeXyMRDTERjxCrrwV6LQxPg7NU9RRwattrj3Y9fg4n7bP9c38DfGuPZRxv7s2efGgdyHIsE4NXbbI1Y3wj5QzQOpZNEg5J4FvqNiK3V+7JkdUiN8x8E7fPh0FbgT5pjAmU1AyUl7ntxhlCIYGCBX1zLdEExCaIVFf463/xHlh+1Xnd0jvG+EM6D0tf4Nd++lbn+W8tw8zNoy3TANmEa/2Qyjl5fLjyO8A1BWMCJZW/ct0CVFYCnZ61mn4/pPNQcaZnpbx85TVjzPhL56G6Cq2mk5qtrwe2jz5Y0O+PVA7Wl5zHneBvNX1j/KFzrVZXoNW4+rUAsvROP3Q3Dzs1/QA3D40JlE6ALy93VdqCe/1aTb8f0m5OX9XJB8YmnBu8xpjx10nFVgrQblz9WgBZ0O+HVB6aNaiXnZpCgJuGxgROp1ZfWXby+t2vBZAF/X7oBPnKcuAHdhgTON3pnXbz6tcCyIJ+P3SaguWCE/gnjo62PMYY7zrrXlQKzo1cCTlToweU3cjth1RXTjDgfXyNCZxw1Jl4rVJwfpIzEApuaLSafj+kt6V3AtzH15hAclfAo90I9E1csKDfH53839p5aFYDnQ80JpBSuSs3cgN+/Qa3DTNM8QyEonD5y85zS+8Y4y+p/JV7chb0zZ5EnCbh8lec5wFvHhoTOJ2xNodgLQxL7/RLqivoW03fGH9JufNntVuBv34t6PdLagZam1ceG2P8I5U7FH30wdI7/dPdJAx489CYwDlE168F/X7pNAlDUefGrjHGP7pTOlbTN550agepXGAXVDYmsLpTslbTN550TpqAnzDGBFLaavpXEZE7ReQVETkrIo/s8P4dIvK8iDRF5N4d3s+IyNdF5D/1o9BjKdVV0zfG+Iuld64QkTDwOHAXcAJ4QERObNvsHPAg8JFddvOrwF8dvJg+0KkpWE3fGP+JpSCacu7HReKjLs1Aeanp3w6cVdXXVbUOPAnc072Bqr6hqi8B7e0fFpFvB44Cz/ShvOPLavrG+FsqdyiuXy9B/zrgfNfzBfe1PYlICPgN4Bf3XzSf6ZwsAR/YYUxgHZKg72Vw1k5dUdTj/n8WOKWq5+UaPVpE5CHgIYAbbrjB467HTDoP7/lX8C0/OuqSGGMO4t3/3JlLP+C8BP0F4Pqu58eBCx73/13Au0XkZ4EJICYiG6p61c1gVX0CeALg5MmTXv+gjBcR+J7gN2iMCawTd4+6BEPhJeg/B9wiIjcBXwfuB/6Bl52r6vs7j0XkQeDk9oBvjDFmePZsy6hqE3gYeBp4Gfi4qp4RkcdE5G4AEblNRBaA+4APiciZQRbaGGPMwYjqeGVTTp48qadPnx51MYwxxldE5HOqenKv7YJ/18IYY8wWC/rGGHOIWNA3xphDxIK+McYcIhb0jTHmEBm73jsichn4Wg+7yAPLfSrOIFj5emPl642VrzfjXL63qOrsXhuNXdDvlYic9tJtaVSsfL2x8vXGytebcS+fF5beMcaYQ8SCvjHGHCJBDPpPjLoAe7Dy9cbK1xsrX2/GvXx7ClxO3xhjzO6CWNM3xhizC18GfQ8LtcdF5GPu+58VkRuHWLbrReRTIvKyiJwRkX+6wzbfKyJFEXnB/Xl0WOXrKsMbIvIF9/hvmuFOHL/tfocvicg7hli2v9P13bwgIiUR+flt2wz1OxSRD4vIJRH5YtdrMyLyrIi86v6e3uWzH3C3eVVEPjDE8n1QRL7s/v/9qYhM7fLZa54LAyzfr4jI17v+D39ol89e83ofYPk+1lW2N0TkhV0+O/Dvr69U1Vc/QBh4DbgZiAEvAie2bfOzwH91H98PfGyI5ZsH3uE+ngS+skP5vhf4ixF/j28A+Wu8/0PAJ3BWTvtO4LMj/P9ewumDPLLvELgDeAfwxa7Xfh14xH38CPBrO3xuBnjd/T3tPp4eUvneC0Tcx7+2U/m8nAsDLN+vAL/g4f//mtf7oMq37f3fAB4d1ffXzx8/1vT3XKjdff577uM/Br5frrVeYx+p6qKqPu8+XsdZg8DTmsJj5h7g99XxGWBKROZHUI7vB15T1V4G7PVMVf8aWNn2cvd59nvAj+zw0R8EnlXVFVVdBZ4F7hxG+VT1GXXWwwD4DM6qdyOxy/fnhZfrvWfXKp8bO34M+Gi/jzsKfgz6XhZq39rGPemLwNBXPHbTSm8HPrvD298lIi+KyCdE5JuHWjCHAs+IyOfcNYq38/I9D8P97H6xjfo7PKqqi+D8sQeO7LDNuHyPP4XTctvJXufCID3spp8+vEt6bBy+v3cDF1X11V3eH+X3t29+DPpeFmrvZTH3vhCRCeB/AD+vqqVtbz+Pk674NuA/An82zLK53qWq7wDuAn5ORO7Y9v44fIcx4G7gj3Z4exy+Qy/G4Xv8JaAJ/OEum+x1LgzKfwG+AXgbsIiTQtlu5N8f8ADXruWP6vs7ED8GfS8LtW9tIyIRIMvBmpYHIiJRnID/h6r6J9vfV9WSqm64j08BURHJD6t87nEvuL8vAX+K04zu5uV7HrS7gOdV9eL2N8bhOwQudlJe7u9LO2wz0u/RvXH8w8D71U1Ab+fhXBgIVb2oqi1VbQO/s8txR/39RYAfBT622zaj+v4Oyo9Bf2uhdrcmeD/w1LZtngI6vSTuBf5ytxO+39z8338HXlbV39xlm7nOPQYRuR3n/6EwjPK5x0yLyGTnMc4Nvy9u2+wp4B+6vXi+Eyh2UhlDtGsNa9Tfoav7PPsA8Oc7bPM08F4RmXbTF+91Xxs4EbkT+JfA3apa2WUbL+fCoMrXfY/ofbsc18v1Pkg/AHxZVRd2enOU39+BjfpO8kF+cHqWfAXnrv4vua89hnNyAyRwUgJngb8Fbh5i2b4bp/n5EvCC+/NDwM8AP+Nu8zBwBqcnwmeAdw75+7vZPfaLbjk632F3GQV43P2OvwCcHHIZUzhBPNv12si+Q5w/PotAA6f2+dM494k+Cbzq/p5xtz0J/Leuz/6Uey6eBX5yiOU7i5MP75yHnR5tx4BT1zoXhlS+P3DPrZdwAvn89vK5z990vQ+jfO7rv9s557q2Hfr3188fG5FrjDGHiB/TO8YYYw7Igr4xxhwiFvSNMeYQsaBvjDGHiAV9Y4w5RCzoG2PMIWJB3xhjDhEL+sYYc4j8f9iSMi9HT4tNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302239, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.308332, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.289100, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.306072, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.250868, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.291086, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.212819, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.325159, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.290994, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.313145, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.238692, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.286604, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.355889, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.233891, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.379898, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.259309, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.315171, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.272651, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.288477, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297109, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.328003, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.322595, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.320111, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.311517, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.309783, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.312030, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305316, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305544, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302871, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303209, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301828, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.305645, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.297752, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.302125, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.301015, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298528, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.298850, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.294343, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.292257, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 2.303052, Train accuracy: 0.196667, val accuracy: 0.206000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.339515, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.324188, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.312022, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.329603, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 2.312070, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.316725, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 2.277508, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 2.233979, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.162042, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.025126, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.906637, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.266989, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.674035, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.259103, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.959786, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.663200, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.053776, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.405266, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.508166, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.805723, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 2.004494, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.568804, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.967230, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.877235, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 1.256787, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.086463, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.735391, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.160559, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 2.015526, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.343634, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.818964, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.652249, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.403237, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.937808, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.686796, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 2.028998, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.096828, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.632072, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.729369, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.901766, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 2.487938, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.128484, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.691020, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.764817, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.478626, Train accuracy: 0.600000, val accuracy: 0.000000\n",
      "Loss: 1.944383, Train accuracy: 0.600000, val accuracy: 0.066667\n",
      "Loss: 1.355046, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.683233, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.686358, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.385972, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.637424, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 1.663101, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.618008, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.877425, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.584734, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.663872, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.076790, Train accuracy: 0.666667, val accuracy: 0.000000\n",
      "Loss: 0.997796, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.555090, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.028639, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 2.067681, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.652520, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.364377, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.377750, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.995498, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.314331, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.276717, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.919653, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.182884, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.782662, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 1.634202, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.235411, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 1.582038, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.745388, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.283189, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.808222, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 2.045547, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.390223, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.981153, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.647631, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.655900, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.155740, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.305531, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.564083, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 0.947727, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.267408, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.300756, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 0.932287, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.379658, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.717858, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.388081, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.648459, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.559791, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.215647, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.300102, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.291681, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.025718, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.726563, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.287155, Train accuracy: 0.800000, val accuracy: 0.000000\n",
      "Loss: 1.616040, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.935106, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.345902, Train accuracy: 0.800000, val accuracy: 0.066667\n",
      "Loss: 1.941323, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.130955, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.328271, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.278687, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.426480, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.603676, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.220900, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.225858, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.647074, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.249827, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.065933, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.311285, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.394974, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.381958, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 1.507334, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.364148, Train accuracy: 0.866667, val accuracy: 0.066667\n",
      "Loss: 1.624832, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.688127, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.199394, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.747265, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.187615, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.717135, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.383422, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.117606, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.177302, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.130658, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 0.977260, Train accuracy: 0.866667, val accuracy: 0.000000\n",
      "Loss: 1.743633, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.696995, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.392860, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.192365, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.162658, Train accuracy: 0.933333, val accuracy: 0.000000\n",
      "Loss: 1.268564, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.380295, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.216389, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.484830, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.368515, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.086129, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.535074, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.401894, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.195483, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.536462, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.383212, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.469463, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.311500, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.361495, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.549090, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 1.499534, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.302475, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 2.300168, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.295515, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 2.286145, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.259617, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 2.158430, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.883044, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: 1.679490, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: 1.814899, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 1.657372, Train accuracy: 0.466667, val accuracy: 0.000000\n",
      "Loss: 1.260702, Train accuracy: 0.533333, val accuracy: 0.000000\n",
      "Loss: 1.095951, Train accuracy: 0.666667, val accuracy: 0.066667\n",
      "Loss: 0.862792, Train accuracy: 0.733333, val accuracy: 0.000000\n",
      "Loss: 0.726604, Train accuracy: 0.733333, val accuracy: 0.066667\n",
      "Loss: 0.540594, Train accuracy: 0.933333, val accuracy: 0.066667\n",
      "Loss: 0.385022, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.250876, Train accuracy: 1.000000, val accuracy: 0.066667\n",
      "Loss: 0.117250, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.037399, Train accuracy: 1.000000, val accuracy: 0.000000\n",
      "Loss: 0.029851, Train accuracy: 1.000000, val accuracy: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-4)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, MomentumSGD(momentum=0.9), learning_rate=4e-1, num_epochs=20, batch_size=20)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a float is required",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-167b1f217e7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Save loss/train/history of the best classifier to the variables above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best validation accuracy achieved: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: a float is required"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_layers: 70.000000, learning_rate: 0.080000, reg_strength: 0.000270\n",
      "Loss: 1.877271, Train accuracy: 0.305889, val accuracy: 0.325000\n",
      "Loss: 1.584825, Train accuracy: 0.539222, val accuracy: 0.488000\n",
      "Loss: 1.452989, Train accuracy: 0.584889, val accuracy: 0.560000\n",
      "Loss: 1.802901, Train accuracy: 0.580667, val accuracy: 0.561000\n",
      "Loss: 1.102700, Train accuracy: 0.620222, val accuracy: 0.605000\n",
      "Loss: 1.606403, Train accuracy: 0.647556, val accuracy: 0.602000\n",
      "Loss: 0.892439, Train accuracy: 0.668778, val accuracy: 0.617000\n",
      "Loss: 1.445456, Train accuracy: 0.697444, val accuracy: 0.655000\n",
      "Loss: 1.164443, Train accuracy: 0.704333, val accuracy: 0.639000\n",
      "Loss: 1.259502, Train accuracy: 0.703222, val accuracy: 0.652000\n",
      "Loss: 1.852982, Train accuracy: 0.645667, val accuracy: 0.615000\n",
      "Loss: 1.495629, Train accuracy: 0.674444, val accuracy: 0.630000\n",
      "Loss: 1.900300, Train accuracy: 0.701778, val accuracy: 0.673000\n",
      "Loss: 0.921521, Train accuracy: 0.700889, val accuracy: 0.645000\n",
      "Loss: 1.487951, Train accuracy: 0.700556, val accuracy: 0.643000\n",
      "Loss: 1.369323, Train accuracy: 0.666444, val accuracy: 0.634000\n",
      "Loss: 1.376381, Train accuracy: 0.670778, val accuracy: 0.638000\n",
      "Loss: 1.312947, Train accuracy: 0.719889, val accuracy: 0.667000\n",
      "Loss: 1.127221, Train accuracy: 0.649222, val accuracy: 0.610000\n",
      "Loss: 1.785785, Train accuracy: 0.697444, val accuracy: 0.650000\n",
      "Loss: 1.363917, Train accuracy: 0.739667, val accuracy: 0.685000\n",
      "Loss: 1.339716, Train accuracy: 0.737667, val accuracy: 0.680000\n",
      "Loss: 1.663015, Train accuracy: 0.710444, val accuracy: 0.648000\n",
      "Loss: 1.441666, Train accuracy: 0.737889, val accuracy: 0.661000\n",
      "Loss: 1.384962, Train accuracy: 0.669000, val accuracy: 0.628000\n",
      "n_layers: 80.000000, learning_rate: 0.080000, reg_strength: 0.000270\n",
      "Loss: 1.752085, Train accuracy: 0.308111, val accuracy: 0.321000\n",
      "Loss: 1.663942, Train accuracy: 0.501000, val accuracy: 0.505000\n",
      "Loss: 1.167336, Train accuracy: 0.597444, val accuracy: 0.601000\n",
      "Loss: 1.280013, Train accuracy: 0.618778, val accuracy: 0.587000\n",
      "Loss: 1.395528, Train accuracy: 0.610333, val accuracy: 0.590000\n",
      "Loss: 1.502502, Train accuracy: 0.664889, val accuracy: 0.648000\n",
      "Loss: 1.313156, Train accuracy: 0.631333, val accuracy: 0.577000\n",
      "Loss: 1.386932, Train accuracy: 0.667778, val accuracy: 0.626000\n",
      "Loss: 1.755168, Train accuracy: 0.701000, val accuracy: 0.666000\n",
      "Loss: 1.404561, Train accuracy: 0.697222, val accuracy: 0.669000\n",
      "Loss: 1.137490, Train accuracy: 0.652444, val accuracy: 0.605000\n",
      "Loss: 1.069991, Train accuracy: 0.690889, val accuracy: 0.657000\n",
      "Loss: 1.501941, Train accuracy: 0.742778, val accuracy: 0.677000\n",
      "Loss: 1.328835, Train accuracy: 0.717778, val accuracy: 0.637000\n",
      "Loss: 1.580558, Train accuracy: 0.676556, val accuracy: 0.631000\n",
      "Loss: 1.175742, Train accuracy: 0.736778, val accuracy: 0.682000\n",
      "Loss: 1.527053, Train accuracy: 0.717333, val accuracy: 0.632000\n",
      "Loss: 1.344332, Train accuracy: 0.712222, val accuracy: 0.650000\n",
      "Loss: 1.413208, Train accuracy: 0.680222, val accuracy: 0.615000\n",
      "Loss: 1.885589, Train accuracy: 0.692333, val accuracy: 0.631000\n",
      "Loss: 1.096311, Train accuracy: 0.681556, val accuracy: 0.620000\n",
      "Loss: 2.272127, Train accuracy: 0.714111, val accuracy: 0.652000\n",
      "Loss: 1.643661, Train accuracy: 0.671667, val accuracy: 0.598000\n",
      "Loss: 1.176181, Train accuracy: 0.692667, val accuracy: 0.616000\n",
      "Loss: 1.385628, Train accuracy: 0.711000, val accuracy: 0.656000\n",
      "n_layers: 90.000000, learning_rate: 0.080000, reg_strength: 0.000270\n",
      "Loss: 1.949567, Train accuracy: 0.293667, val accuracy: 0.296000\n",
      "Loss: 1.754258, Train accuracy: 0.496667, val accuracy: 0.488000\n",
      "Loss: 1.599375, Train accuracy: 0.583000, val accuracy: 0.585000\n",
      "Loss: 0.839612, Train accuracy: 0.637667, val accuracy: 0.590000\n",
      "Loss: 1.809223, Train accuracy: 0.627000, val accuracy: 0.601000\n",
      "Loss: 1.888896, Train accuracy: 0.652000, val accuracy: 0.625000\n",
      "Loss: 1.288061, Train accuracy: 0.653667, val accuracy: 0.619000\n",
      "Loss: 1.708418, Train accuracy: 0.647556, val accuracy: 0.611000\n",
      "Loss: 1.293711, Train accuracy: 0.609556, val accuracy: 0.590000\n",
      "Loss: 1.085472, Train accuracy: 0.693778, val accuracy: 0.635000\n",
      "Loss: 0.914728, Train accuracy: 0.707556, val accuracy: 0.645000\n",
      "Loss: 1.564391, Train accuracy: 0.686889, val accuracy: 0.651000\n",
      "Loss: 0.880169, Train accuracy: 0.716111, val accuracy: 0.663000\n",
      "Loss: 1.879559, Train accuracy: 0.621111, val accuracy: 0.589000\n",
      "Loss: 2.198639, Train accuracy: 0.726111, val accuracy: 0.647000\n",
      "Loss: 1.946889, Train accuracy: 0.671889, val accuracy: 0.619000\n",
      "Loss: 1.339853, Train accuracy: 0.710778, val accuracy: 0.660000\n",
      "Loss: 2.310067, Train accuracy: 0.665000, val accuracy: 0.603000\n",
      "Loss: 1.579253, Train accuracy: 0.739889, val accuracy: 0.681000\n",
      "Loss: 1.009509, Train accuracy: 0.685444, val accuracy: 0.630000\n",
      "Loss: 1.026327, Train accuracy: 0.729333, val accuracy: 0.669000\n",
      "Loss: 1.587466, Train accuracy: 0.716222, val accuracy: 0.654000\n",
      "Loss: 1.201427, Train accuracy: 0.692889, val accuracy: 0.646000\n",
      "Loss: 1.015403, Train accuracy: 0.753556, val accuracy: 0.680000\n",
      "Loss: 1.767599, Train accuracy: 0.710444, val accuracy: 0.648000\n",
      "n_layers: 100.000000, learning_rate: 0.080000, reg_strength: 0.000270\n",
      "Loss: 1.894117, Train accuracy: 0.340222, val accuracy: 0.328000\n",
      "Loss: 1.299882, Train accuracy: 0.567667, val accuracy: 0.554000\n",
      "Loss: 1.302908, Train accuracy: 0.544889, val accuracy: 0.534000\n",
      "Loss: 1.088855, Train accuracy: 0.648222, val accuracy: 0.627000\n",
      "Loss: 2.075259, Train accuracy: 0.602778, val accuracy: 0.579000\n",
      "Loss: 1.948637, Train accuracy: 0.621667, val accuracy: 0.571000\n",
      "Loss: 1.368336, Train accuracy: 0.620778, val accuracy: 0.584000\n",
      "Loss: 1.554877, Train accuracy: 0.653333, val accuracy: 0.626000\n",
      "Loss: 1.186171, Train accuracy: 0.642111, val accuracy: 0.622000\n",
      "Loss: 1.531714, Train accuracy: 0.622889, val accuracy: 0.578000\n",
      "Loss: 1.262962, Train accuracy: 0.650111, val accuracy: 0.593000\n",
      "Loss: 1.610609, Train accuracy: 0.694556, val accuracy: 0.646000\n",
      "Loss: 2.039055, Train accuracy: 0.703444, val accuracy: 0.643000\n",
      "Loss: 1.294607, Train accuracy: 0.719222, val accuracy: 0.668000\n",
      "Loss: 1.263885, Train accuracy: 0.710889, val accuracy: 0.655000\n",
      "Loss: 1.059782, Train accuracy: 0.726556, val accuracy: 0.655000\n",
      "Loss: 2.283555, Train accuracy: 0.677222, val accuracy: 0.622000\n",
      "Loss: 1.405189, Train accuracy: 0.652000, val accuracy: 0.614000\n",
      "Loss: 1.538246, Train accuracy: 0.721667, val accuracy: 0.653000\n",
      "Loss: 1.386895, Train accuracy: 0.669667, val accuracy: 0.620000\n",
      "Loss: 2.141951, Train accuracy: 0.705444, val accuracy: 0.659000\n",
      "Loss: 2.125654, Train accuracy: 0.748000, val accuracy: 0.681000\n",
      "Loss: 0.939006, Train accuracy: 0.712556, val accuracy: 0.633000\n",
      "Loss: 1.238464, Train accuracy: 0.705333, val accuracy: 0.637000\n",
      "Loss: 2.047576, Train accuracy: 0.707222, val accuracy: 0.632000\n",
      "n_layers: 70.000000, learning_rate: 0.080000, reg_strength: -0.300000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gulnur/Desktop/workspace/dl_course/assignments/assignment2/layers.py:57: RuntimeWarning: divide by zero encountered in log\n",
      "  result = np.mean(-np.log(probs[rows, cols]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: inf, Train accuracy: 0.342111, val accuracy: 0.315000\n",
      "Loss: inf, Train accuracy: 0.370778, val accuracy: 0.360000\n",
      "Loss: inf, Train accuracy: 0.326111, val accuracy: 0.291000\n",
      "Loss: inf, Train accuracy: 0.337667, val accuracy: 0.342000\n",
      "Loss: inf, Train accuracy: 0.371889, val accuracy: 0.380000\n",
      "Loss: inf, Train accuracy: 0.424556, val accuracy: 0.404000\n",
      "Loss: inf, Train accuracy: 0.398556, val accuracy: 0.381000\n",
      "Loss: inf, Train accuracy: 0.427222, val accuracy: 0.403000\n",
      "Loss: inf, Train accuracy: 0.419333, val accuracy: 0.415000\n",
      "Loss: inf, Train accuracy: 0.432778, val accuracy: 0.410000\n",
      "Loss: inf, Train accuracy: 0.469778, val accuracy: 0.442000\n",
      "Loss: inf, Train accuracy: 0.417556, val accuracy: 0.382000\n",
      "Loss: inf, Train accuracy: 0.436000, val accuracy: 0.413000\n",
      "Loss: inf, Train accuracy: 0.415556, val accuracy: 0.384000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gulnur/anaconda/envs/py35/lib/python3.5/site-packages/numpy/core/fromnumeric.py:1468: RuntimeWarning: overflow encountered in reduce\n",
      "  return asanyarray(a).trace(offset=offset, axis1=axis1, axis2=axis2, dtype=dtype, out=out)\n",
      "/Users/gulnur/Desktop/workspace/dl_course/assignments/assignment2/model.py:73: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  loss += l2_reg\n",
      "/Users/gulnur/Desktop/workspace/dl_course/assignments/assignment2/layers.py:24: RuntimeWarning: invalid value encountered in subtract\n",
      "  predictions_= predictions - maxim\n",
      "/Users/gulnur/Desktop/workspace/dl_course/assignments/assignment2/layers.py:139: RuntimeWarning: invalid value encountered in greater\n",
      "  self.d_out_result = np.greater(X, 0).astype(float)\n",
      "/Users/gulnur/Desktop/workspace/dl_course/assignments/assignment2/layers.py:140: RuntimeWarning: invalid value encountered in maximum\n",
      "  return np.maximum(X, 0)\n",
      "/Users/gulnur/anaconda/envs/py35/lib/python3.5/site-packages/numpy/core/fromnumeric.py:83: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "n_layers: 80.000000, learning_rate: 0.080000, reg_strength: -0.300000\n",
      "Loss: inf, Train accuracy: 0.249111, val accuracy: 0.221000\n",
      "Loss: inf, Train accuracy: 0.394778, val accuracy: 0.375000\n",
      "Loss: inf, Train accuracy: 0.389667, val accuracy: 0.366000\n",
      "Loss: inf, Train accuracy: 0.355556, val accuracy: 0.337000\n",
      "Loss: inf, Train accuracy: 0.384778, val accuracy: 0.377000\n",
      "Loss: inf, Train accuracy: 0.406667, val accuracy: 0.407000\n",
      "Loss: inf, Train accuracy: 0.483556, val accuracy: 0.457000\n",
      "Loss: inf, Train accuracy: 0.468333, val accuracy: 0.426000\n",
      "Loss: inf, Train accuracy: 0.491778, val accuracy: 0.462000\n",
      "Loss: inf, Train accuracy: 0.487222, val accuracy: 0.456000\n",
      "Loss: inf, Train accuracy: 0.512444, val accuracy: 0.445000\n",
      "Loss: inf, Train accuracy: 0.499889, val accuracy: 0.476000\n",
      "Loss: inf, Train accuracy: 0.488333, val accuracy: 0.476000\n",
      "Loss: inf, Train accuracy: 0.487333, val accuracy: 0.456000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "n_layers: 90.000000, learning_rate: 0.080000, reg_strength: -0.300000\n",
      "Loss: inf, Train accuracy: 0.342667, val accuracy: 0.323000\n",
      "Loss: inf, Train accuracy: 0.421333, val accuracy: 0.384000\n",
      "Loss: inf, Train accuracy: 0.430889, val accuracy: 0.353000\n",
      "Loss: inf, Train accuracy: 0.350778, val accuracy: 0.335000\n",
      "Loss: inf, Train accuracy: 0.441667, val accuracy: 0.405000\n",
      "Loss: inf, Train accuracy: 0.439556, val accuracy: 0.410000\n",
      "Loss: inf, Train accuracy: 0.437556, val accuracy: 0.399000\n",
      "Loss: inf, Train accuracy: 0.454444, val accuracy: 0.413000\n",
      "Loss: inf, Train accuracy: 0.510889, val accuracy: 0.486000\n",
      "Loss: inf, Train accuracy: 0.540222, val accuracy: 0.502000\n",
      "Loss: inf, Train accuracy: 0.524889, val accuracy: 0.450000\n",
      "Loss: inf, Train accuracy: 0.503444, val accuracy: 0.478000\n",
      "Loss: inf, Train accuracy: 0.564778, val accuracy: 0.520000\n",
      "Loss: inf, Train accuracy: 0.531444, val accuracy: 0.457000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n",
      "Loss: nan, Train accuracy: 0.066333, val accuracy: 0.049000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-f963dd38487d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMomentumSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val_accuracy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/dl_course/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/dl_course/assignments/assignment2/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m#regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0ml2_W1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_W1_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0ml2_B1_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_B1_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0ml2_W2_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml2_W2_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/dl_course/assignments/assignment2/layers.py\u001b[0m in \u001b[0;36ml2_regularization\u001b[0;34m(W, reg_strength)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# TODO: Copy from the previous assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreg_strength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreg_strength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "learning_rates = [0.08, 1e-1, 1e-2, 1e-3]\n",
    "reg_strength = [0.00027,-3e-1, 3e-1, 3e-2]\n",
    "learning_rate_decay = 0.9\n",
    "hidden_layer_size = [70, 80, 90, 100]\n",
    "num_epochs = 25\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "for lr, reg, n_layers in itertools.product(learning_rates, \n",
    "                                           reg_strength,\n",
    "                                           hidden_layer_size):\n",
    "    #print(f'learning_rate={lr}, reg_strength={reg}, n_layers={n_layers}')\n",
    "    print(\"n_layers: %f, learning_rate: %f, reg_strength: %f\" %\n",
    "                  (n_layers, lr, reg))\n",
    "    \n",
    "    model = TwoLayerNet(n_input=train_X.shape[1], n_output=10, hidden_layer_size=n_layers, reg=reg)\n",
    "    dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "    trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=lr, num_epochs=num_epochs, batch_size=batch_size)\n",
    "    loss_hist, train_hist, val_hist = trainer.fit()\n",
    "    accuracy = val_hist[-1]\n",
    "    if not best_val_accuracy or accuracy > best_val_accuracy:\n",
    "        best_classifier = model\n",
    "        best_val_accuracy = accuracy\n",
    "        loss_history = loss_hist\n",
    "        train_history = train_hist\n",
    "        val_history = val_hist\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.431431, Train accuracy: 0.323667, val accuracy: 0.342000\n",
      "Loss: 1.971650, Train accuracy: 0.525222, val accuracy: 0.546000\n",
      "Loss: 1.501104, Train accuracy: 0.602111, val accuracy: 0.589000\n",
      "Loss: 1.556687, Train accuracy: 0.647444, val accuracy: 0.628000\n",
      "Loss: 1.260826, Train accuracy: 0.612111, val accuracy: 0.584000\n",
      "Loss: 1.100646, Train accuracy: 0.635778, val accuracy: 0.594000\n",
      "Loss: 1.586974, Train accuracy: 0.661556, val accuracy: 0.634000\n",
      "Loss: 0.982162, Train accuracy: 0.688333, val accuracy: 0.645000\n",
      "Loss: 1.199528, Train accuracy: 0.695444, val accuracy: 0.652000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-add6251751b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                         batch_size=batch_size)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/workspace/dl_course/assignments/assignment2/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/dl_course/assignments/assignment2/model.py\u001b[0m in \u001b[0;36mcompute_loss_and_gradients\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m#forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mout_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mout_relu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_layer1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mout_layer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_relu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/workspace/dl_course/assignments/assignment2/layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;31m#raise Exception(\"Not implemented!\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "learning_rate_10pow = -1.099643\n",
    "reg_strength_10pow = -3.570692\n",
    "learning_rate_decay = 0.964509\n",
    "hidden_layer_size = 81\n",
    "num_epochs = 200\n",
    "batch_size = 53\n",
    "\n",
    "learning_rate = math.pow(10.0, learning_rate_10pow)\n",
    "reg_strength = math.pow(10.0, reg_strength_10pow)\n",
    "\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "\n",
    "nn = TwoLayerNet(n_input=train_X.shape[1], n_output=10, \n",
    "                       hidden_layer_size=hidden_layer_size, \n",
    "                       reg=reg_strength)\n",
    "\n",
    "coach = Trainer(nn, dataset, MomentumSGD(),\n",
    "                        learning_rate=learning_rate, \n",
    "                        learning_rate_decay=learning_rate_decay, \n",
    "                        num_epochs=num_epochs, \n",
    "                        batch_size=batch_size)\n",
    "\n",
    "loss_history, train_accuracy, val_accuracy = coach.fit();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10e2c4668>]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.791000\n"
     ]
    }
   ],
   "source": [
    "best_classifier = nn\n",
    "best_val_accuracy = val_accuracy[np.argmax(val_accuracy)]\n",
    "\n",
    "# find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural net test set accuracy: 0.771000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
