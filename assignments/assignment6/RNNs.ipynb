{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfbnblO3NJ_o",
        "colab_type": "text"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P59NYU98GCb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "outputId": "f1613055-10b1-4a01-9a8b-0847160e9b7e"
      },
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 519.5MB 30kB/s \n",
            "\u001b[31mERROR: torchvision 0.6.0+cu101 has requirement torch==1.5.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.60 has requirement torch>=1.0.0, but you'll have torch 0.4.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 16.0MB 214kB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 113kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8sVtGHmA9aBM",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiA2dGmgF1rW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "96c88682-0c43-423a-d775-63c923aaf304"
      },
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QstS4NO0L97c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "a1cf46cc-1338-41e4-8178-0334f9856f1c"
      },
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xTai8Ta0lgwL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "caad7fd5-4880-4224-b22e-991269a6e1f9"
      },
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pCjwwDs6Zq9x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "58c1030f-2b04-4ce7-fcf8-fd8e832e461f"
      },
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words in train = 45441. Tags = {'NUM', 'PRT', 'ADP', 'ADJ', '.', 'DET', 'VERB', 'ADV', 'X', 'CONJ', 'NOUN', 'PRON'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "URC1B2nvPGFt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "b923dd2b-40e9-404a-da62-2f08cc15612e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAdb0lEQVR4nO3dfbRldX3f8fcnM8VlkhpQJsTw4KAOKlgzkVnKSjRRER1MlmAWUWgig6WOLmGlUpuKSVps1BRN7HTRKC6MEyA1PERioK4xOEWNphVlEORBBQZEmSkCAZUmWhX89o/zu7jncu7Mnfv4u5f3a62zzj7fvX/7fPedc+d87n44J1WFJEmS+vITi92AJEmSHs2QJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktShlYvdwFzbf//9a/Xq1YvdhiRJ0h5de+21/1BVq8bNW3YhbfXq1Wzbtm2x25AkSdqjJF+fap6HOyVJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDu0xpCXZnOTeJDcNapckub7d7kxyfauvTvK9wbwPDMYcmeTGJNuTnJMkrf7EJFuT3Nbu92v1tOW2J7khyXPnfvMlSZL6NJ09aecD64eFqnpNVa2tqrXAZcBfD2bfPjGvqt44qJ8LvB5Y024T6zwTuKqq1gBXtccAxw6W3djGS5IkPSbsMaRV1WeAB8bNa3vDXg1ctLt1JHky8ISqurqqCrgQOL7NPg64oE1fMKl+YY1cDezb1iNJkrTszfa7O18I3FNVtw1qhya5DngQ+IOq+ixwILBjsMyOVgM4oKrubtPfBA5o0wcCd40ZczeSJGlBbdp666zGn3HMYXPUyWPHbEPaSey6F+1u4JCquj/JkcDfJDliuiurqkpSe9tEko2MDolyyCGH7O1wSZKk7sz46s4kK4HfAC6ZqFXV96vq/jZ9LXA7cBiwEzhoMPygVgO4Z+IwZru/t9V3AgdPMWYXVXVeVa2rqnWrVq2a6SZJkiR1YzYfwfFS4KtV9chhzCSrkqxo009ldNL/He1w5oNJjmrnsZ0MXN6GXQFsaNMbJtVPbld5HgV8Z3BYVJIkaVmbzkdwXAR8DnhGkh1JTm2zTuTRFwz8CnBD+0iOjwBvrKqJiw7eBPwZsJ3RHraPt/rZwDFJbmMU/M5u9S3AHW35D7bxkiRJjwl7PCetqk6aon7KmNpljD6SY9zy24Bnj6nfDxw9pl7AaXvqT5IkaTnyGwckSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDu0xpCXZnOTeJDcNam9PsjPJ9e32isG8tyXZnuSWJC8f1Ne32vYkZw7qhyb5fKtfkmSfVn9ce7y9zV89VxstSZLUu+nsSTsfWD+mvqmq1rbbFoAkhwMnAke0Me9PsiLJCuB9wLHA4cBJbVmAd7d1PR34FnBqq58KfKvVN7XlJEmSHhP2GNKq6jPAA9Nc33HAxVX1/ar6GrAdeF67ba+qO6rqB8DFwHFJArwE+EgbfwFw/GBdF7TpjwBHt+UlSZKWvdmck3Z6khva4dD9Wu1A4K7BMjtabar6k4BvV9VDk+q7rKvN/05bXpIkadmbaUg7F3gasBa4G3jvnHU0A0k2JtmWZNt99923mK1IkiTNiRmFtKq6p6oerqofAR9kdDgTYCdw8GDRg1ptqvr9wL5JVk6q77KuNv9n2vLj+jmvqtZV1bpVq1bNZJMkSZK6MqOQluTJg4evAiau/LwCOLFdmXkosAb4AnANsKZdybkPo4sLrqiqAj4FnNDGbwAuH6xrQ5s+AfhkW16SJGnZW7mnBZJcBLwI2D/JDuAs4EVJ1gIF3Am8AaCqbk5yKfBl4CHgtKp6uK3ndOBKYAWwuapubk/xVuDiJO8ErgM+1OofAv4iyXZGFy6cOOutlSRJWiL2GNKq6qQx5Q+NqU0s/y7gXWPqW4AtY+p38OPDpcP6/wN+c0/9SZIkLUd+44AkSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUoT2GtCSbk9yb5KZB7Y+TfDXJDUk+mmTfVl+d5HtJrm+3DwzGHJnkxiTbk5yTJK3+xCRbk9zW7vdr9bTltrfnee7cb74kSVKfprMn7Xxg/aTaVuDZVfUc4FbgbYN5t1fV2nZ746B+LvB6YE27TazzTOCqqloDXNUeAxw7WHZjGy9JkvSYsMeQVlWfAR6YVPtEVT3UHl4NHLS7dSR5MvCEqrq6qgq4EDi+zT4OuKBNXzCpfmGNXA3s29YjSZK07M3FOWn/Cvj44PGhSa5L8ndJXthqBwI7BsvsaDWAA6rq7jb9TeCAwZi7phgjSZK0rK2czeAkvw88BHy4le4GDqmq+5McCfxNkiOmu76qqiQ1gz42MjokyiGHHLK3wyVJkroz4z1pSU4Bfh34rXYIk6r6flXd36avBW4HDgN2sush0YNaDeCeicOY7f7eVt8JHDzFmF1U1XlVta6q1q1atWqmmyRJktSNGYW0JOuBfw+8sqq+O6ivSrKiTT+V0Un/d7TDmQ8mOapd1XkycHkbdgWwoU1vmFQ/uV3leRTwncFhUUmSpGVtj4c7k1wEvAjYP8kO4CxGV3M+DtjaPknj6nYl568Af5jkh8CPgDdW1cRFB29idKXo4xmdwzZxHtvZwKVJTgW+Dry61bcArwC2A98FXjebDZUkSVpK9hjSquqkMeUPTbHsZcBlU8zbBjx7TP1+4Ogx9QJO21N/kiRJy5HfOCBJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHZrVd3dKkvZs09ZbZzz2jGMOm8NOJC0l7kmTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjo0rZCWZHOSe5PcNKg9McnWJLe1+/1aPUnOSbI9yQ1JnjsYs6Etf1uSDYP6kUlubGPOSZLdPYckSdJyN909aecD6yfVzgSuqqo1wFXtMcCxwJp22wicC6PABZwFPB94HnDWIHSdC7x+MG79Hp5DkiRpWZtWSKuqzwAPTCofB1zQpi8Ajh/UL6yRq4F9kzwZeDmwtaoeqKpvAVuB9W3eE6rq6qoq4MJJ6xr3HJIkScvabM5JO6Cq7m7T3wQOaNMHAncNltvRarur7xhT391z7CLJxiTbkmy77777Zrg5kiRJ/ZiTCwfaHrCai3XN5Dmq6ryqWldV61atWjWfbUiSJC2I2YS0e9qhStr9va2+Ezh4sNxBrba7+kFj6rt7DkmSpGVtNiHtCmDiCs0NwOWD+sntKs+jgO+0Q5ZXAi9Lsl+7YOBlwJVt3oNJjmpXdZ48aV3jnkOSJGlZWzmdhZJcBLwI2D/JDkZXaZ4NXJrkVODrwKvb4luAVwDbge8CrwOoqgeSvAO4pi33h1U1cTHCmxhdQfp44OPtxm6eQ5IkaVmbVkirqpOmmHX0mGULOG2K9WwGNo+pbwOePaZ+/7jnkCRJWu78xgFJkqQOGdIkSZI6ZEiTJEnq0LTOSZO0PG3aeuusxp9xzGFz1IkkaTL3pEmSJHXIkCZJktQhD3eqW7M5FOdhOEnSUueeNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkJ+TJmlJ8ausJD1WuCdNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnq0IxDWpJnJLl+cHswyZuTvD3JzkH9FYMxb0uyPcktSV4+qK9vte1JzhzUD03y+Va/JMk+M99USZKkpWPGIa2qbqmqtVW1FjgS+C7w0TZ708S8qtoCkORw4ETgCGA98P4kK5KsAN4HHAscDpzUlgV4d1vX04FvAafOtF9JkqSlZK4Odx4N3F5VX9/NMscBF1fV96vqa8B24Hnttr2q7qiqHwAXA8clCfAS4CNt/AXA8XPUryRJUtfmKqSdCFw0eHx6khuSbE6yX6sdCNw1WGZHq01VfxLw7ap6aFJdkiRp2Zt1SGvnib0S+KtWOhd4GrAWuBt472yfYxo9bEyyLcm2++67b76fTpIkad7NxZ60Y4EvVtU9AFV1T1U9XFU/Aj7I6HAmwE7g4MG4g1ptqvr9wL5JVk6qP0pVnVdV66pq3apVq+ZgkyRJkhbXXIS0kxgc6kzy5MG8VwE3tekrgBOTPC7JocAa4AvANcCadiXnPowOnV5RVQV8Cjihjd8AXD4H/UqSJHVv5Z4XmVqSnwKOAd4wKL8nyVqggDsn5lXVzUkuBb4MPAScVlUPt/WcDlwJrAA2V9XNbV1vBS5O8k7gOuBDs+lXkiRpqZhVSKuqf2J0gv+w9trdLP8u4F1j6luALWPqd/Djw6WSJEmPGX7jgCRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHVo5WI3IC0nm7beOuOxZxxz2Bx2Ikla6ma9Jy3JnUluTHJ9km2t9sQkW5Pc1u73a/UkOSfJ9iQ3JHnuYD0b2vK3JdkwqB/Z1r+9jc1se5YkSerdXB3ufHFVra2qde3xmcBVVbUGuKo9BjgWWNNuG4FzYRTqgLOA5wPPA86aCHZtmdcPxq2fo54lSZK6NV/npB0HXNCmLwCOH9QvrJGrgX2TPBl4ObC1qh6oqm8BW4H1bd4TqurqqirgwsG6JEmSlq25CGkFfCLJtUk2ttoBVXV3m/4mcECbPhC4azB2R6vtrr5jTF2SJGlZm4sLB15QVTuT/CywNclXhzOrqpLUHDzPlFo43AhwyCGHzOdTSZIkLYhZ70mrqp3t/l7go4zOKbunHaqk3d/bFt8JHDwYflCr7a5+0Jj65B7Oq6p1VbVu1apVs90kSZKkRTerkJbkp5L884lp4GXATcAVwMQVmhuAy9v0FcDJ7SrPo4DvtMOiVwIvS7Jfu2DgZcCVbd6DSY5qV3WePFiXJEnSsjXbw50HAB9tn4qxEvjLqvrbJNcAlyY5Ffg68Oq2/BbgFcB24LvA6wCq6oEk7wCuacv9YVU90KbfBJwPPB74eLtJkiQta7MKaVV1B/ALY+r3A0ePqRdw2hTr2gxsHlPfBjx7Nn1KkiQtNX4tlCRJUocMaZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR1yJAmSZLUIUOaJElShwxpkiRJHTKkSZIkdciQJkmS1CFDmiRJUocMaZIkSR0ypEmSJHXIkCZJktShlYvdgCSpL5u23jqr8Wccc9gcdSI9trknTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQO+REckiRpWVrqHyfjnjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQzMOaUkOTvKpJF9OcnOSf9Pqb0+yM8n17faKwZi3Jdme5JYkLx/U17fa9iRnDuqHJvl8q1+SZJ+Z9itJkrSUzGZP2kPAW6rqcOAo4LQkh7d5m6pqbbttAWjzTgSOANYD70+yIskK4H3AscDhwEmD9by7revpwLeAU2fRryRJ0pIx45BWVXdX1Rfb9P8FvgIcuJshxwEXV9X3q+prwHbgee22varuqKofABcDxyUJ8BLgI238BcDxM+1XkiRpKZmTc9KSrAZ+Efh8K52e5IYkm5Ps12oHAncNhu1otanqTwK+XVUPTapLkiQte7MOaUl+GrgMeHNVPQicCzwNWAvcDbx3ts8xjR42JtmWZNt99903308nSZI072b1jQNJ/hmjgPbhqvprgKq6ZzD/g8DH2sOdwMGD4Qe1GlPU7wf2TbKy7U0bLr+LqjoPOA9g3bp1NZttkiRpIczm0/AX+5PwtTBmc3VngA8BX6mq/zKoP3mw2KuAm9r0FcCJSR6X5FBgDfAF4BpgTbuScx9GFxdcUVUFfAo4oY3fAFw+034lSZKWktnsSftl4LXAjUmub7XfY3R15lqggDuBNwBU1c1JLgW+zOjK0NOq6mGAJKcDVwIrgM1VdXNb31uBi5O8E7iOUSiUJEla9mYc0qrq74GMmbVlN2PeBbxrTH3LuHFVdQejqz8lSZIeU/zGAUmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDs/qcNC0ds/k8HvAzeSRJWmjuSZMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ4Y0SZKkDhnSJEmSOmRIkyRJ6pAhTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSerQysVuYCnatPXWWY0/45jD5qgTSZK0XLknTZIkqUOGNEmSpA4Z0iRJkjpkSJMkSeqQIU2SJKlDhjRJkqQOGdIkSZI6ZEiTJEnqkCFNkiSpQ92HtCTrk9ySZHuSMxe7H0mSpIXQdUhLsgJ4H3AscDhwUpLDF7crSZKk+dd1SAOeB2yvqjuq6gfAxcBxi9yTJEnSvOv9C9YPBO4aPN4BPH+RepEkdWrT1ltnNf6MYw6bo06kuZOqWuweppTkBGB9Vf3r9vi1wPOr6vRJy20ENraHzwBuWdBGH21/4B8WuYe9Zc/zb6n1C/a8EJZav2DPC2Wp9bzU+oU+en5KVa0aN6P3PWk7gYMHjw9qtV1U1XnAeQvV1J4k2VZV6xa7j71hz/NvqfUL9rwQllq/YM8LZan1vNT6hf577v2ctGuANUkOTbIPcCJwxSL3JEmSNO+63pNWVQ8lOR24ElgBbK6qmxe5LUmSpHnXdUgDqKotwJbF7mMvdXPodS/Y8/xbav2CPS+EpdYv2PNCWWo9L7V+ofOeu75wQJIk6bGq93PSJEmSHpMMaXshSSV57+Dxv0vy9jZ9fvvIkOHy/9juV7ex7xzM2z/JD5P86QL1/nCS65PclOSvkvzkmPr/SLJvks+32jeS3Nemr0+yeiF6bX0d335mz2yPVyf5XpLrknwlyReSnDJY/pRBr19O8vqF6nWOel+Q18FSN3i93pzkS0nekuQn2rwXJfnO4PV6fZLXDKa/mWTn4PE+89Dfp5K8fFLtzUk+3l4Dw95ObvPvTHJjkhuS/F2Sp4zZ3i8l+WKSX5rrnqfYjmm/hpP8apLPTRq/Msk9SX5+IfqdSpKDk3wtyRPb4/3a49WL2NPPJbk4ye1Jrk2yJclhSY5I8smMvgbxtiT/IUnamFOS/CjJcwbruWliO9praP857HHK97r2eGOSr7bbF5K8YDBvl17a7+XHprMdc9D3tN/nBmNm/HNfCIa0vfN94Ddm+MvwNeDXBo9/E1jIiyC+V1Vrq+rZwA+AN46pPwCcVlXPr6q1wH8ELmnz11bVnQvY70nA37f7CbdX1S9W1bMYXen75iSvG8y/pPX9IuCPkhywYN3uaia9a3omXq9HAMcw+sq4swbzPzt4va6tqkdev8AHgE2DeT+Yh/4uYvTvO3Qi8J8ZvQaGvV04WObFVfUc4NPAHwzqE9v7C8Db2noWwt68hj8LHDQMl8BLgZur6v8sUL9jVdVdwLnA2a10NnDeAv9f9oj25v9R4NNV9bSqOpLRv+sBjD654OyqegbwC8AvAW8aDN8B/P4CtTrle12SXwfeALygqp7J6L3kL5P83DTXPZ/bMe33OYAkj6evn/ujGNL2zkOMTjI8YwZjvwt8JcnE57G8Brh0rhrbS58Fnj6m/jlG3/KwqJL8NPAC4FQe/YYHQFXdAfxb4HfGzLsXuB14yuR58222vWv62r/zRuD0ib98O/AR4Ncm9tK1v7h/nl2/OWV3dvc7+ATgW7Psb4/29jVcVT9i9H/ZcNkTGQXWHmwCjkryZkbb9SeL2MuLgR9W1QcmClX1JeAw4H9V1Sda7bvA6cCZg7EfA45I8owF6HN373VvBX63qv4BoKq+CFxACz7TsFDbMZ33uX9JXz/3RzGk7b33Ab+V5GdmMPZi4MQkBwMPAwv+V2aSlYz2Ptw4qb4COJo+PofuOOBvq+pW4P4kR06x3BeBZ04uJnkq8FRg+/y1OKVZ9a6908LCCuBnW+mFkw4pPm2B+3kA+AKj3zEYhZVLgQKeNqm3F45ZxXrgbwaPH9+W/SrwZ8A75rH9CTN5DT+yBzHJ44BXAJfNd6PTUVU/BH6XUVh7c3u8WJ4NXDumfsTkelXdDvx0kie00o+A9wC/N68d/thU73WP6hXY1urTMe/bsRfvcz3+3HdhSNtLVfUgcCGP3gsy7jLZybW/ZXSI5kTgkrnvbrcen+R6Rr9M3wA+NKn+TUa73LcucF/jnMQo0NLuT5piucl7T17TtuUi4A3tDXOhzbR3zY3JhztvX4Qehoc8h3uUJh/u/OxgzKeS7GT0xjLcAzVxmOaZjALchQuw13CvX8NVtY3RG9szGG3D5xfp928qxwJ3MwpJS9lfMtoreOh8P9Fu3uv2OHQatfnajvl6n1uwn/tk3X9OWqf+K6O/Iv98ULsf2G/iQTtRdZfvA6uqHyS5FngLcDjwyvlv9RHfa+fljK23EyyvZLTL+pwF7GsX7ef2EuBfJClGe0mK0V91k/0i8JXB40smf6/rQppl75qBttf0YeBe4FmL3M6Ey4FNSZ4L/GRVXTuNE41fDHwb+DDwnxgdStxFVX2unSO0itH2zrlZvoYnwumz6OdQJ0nWMvrj+Cjg75NcXFV3L1I7NwMnjKl/GfiVYaG9tv+xqh6cyOXtA97fy+iQ40IY9173ZeBI4JOD2pH8+BzriffCife/ce+F87Ude/s+1+vP/RHuSZuB9hfipYzO2ZjwaUZ7ciauGDsF+NSY4e8F3trZX5kTx+J/B3hL21W8WE4A/qKqnlJVq6vqYEYXXQy/w3XiXJ8/Af7bgnc4taXc+5KTZBWjiwH+tDr6wMeq+kdGv/ub2YuwUlUPAW8GTm5haRcZXWm5gtGb4HyZzWv4IuC3GYW8y+exx2lrex3PZXSY8xvAH7O456R9Enhcko0ThXbl4C3AC5K8tNUezyhEvGfMOs5ndGHG2C/knktTvNe9B3h3kifBIyH4FOD9bf6ngde2eSsYvSbGvReezwJtx4Qx73MfpsOf+5AhbebeCzxy5UtVfYzRiYrXtt2qv8yY1F1VN1fVBQvW5V6oquuAG5j68MZCOInR1U9DlzG6AuppaR8BwOg/jnOq6s8nr2ARzbT3lYyupupORh8PsKgfozDJxDlaNwP/E/gEoz1PEyafkzZur8VCuIjRlWLDkDb5nLRxF73c3cZMnIQ9sb3XMzpFYkNVPTyPfc/496+qvgL8E/DJqvqneexxb7we+EZVTRzeej/wrCS/uhjNtD8mXgW8NKOP4LiZ0RW732R0LuAfJLmF0blU1wCP+miedlXyOfz4PEyY3/9DJr/XXcHoD5D/3c6V/CDw24O9k+8Anp7kS8B1jM4N/u+TVzrFdsy74ftcVX2P2f3c553fOCAtsiSbgNuq6v17XFiSBtoe5euratGvzNfcc0+atIiSfBx4DqPd7pI0bUleyegIztsWuxfND/ekSZIkdcg9aZIkSR0ypEmSJHXIkCZJktQhQ5okSVKHDGmSJEkdMqRJkiR16P8DZX+EtduMAHAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5rWmSToIaeAo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "525da916-42c0-4d9a-e00d-6e886234ecd1"
      },
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vjz_Rk0bbMyH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c14105a5-5456-4bee-8ca8-c62fd7acceb5"
      },
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8XCuxEBVbOY_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c3074273-7ef7-404c-9c8a-e4e204907d92"
      },
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RtRbz1SwgEqc",
        "colab": {}
      },
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DhsTKZalfih6",
        "colab": {}
      },
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4XsRII5kW5x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "caf02f29-33aa-4b14-86fb-bd96ae9d3a1b"
      },
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WVEHju54d68T",
        "colab": {}
      },
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1, verbose=False):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self.linear = nn.Linear(in_features=lstm_hidden_dim, out_features=tagset_size)    \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.verbose:\n",
        "            print(f'Input shape: {inputs.shape}')\n",
        "            \n",
        "        embedding_out = self.embedding(inputs)\n",
        "        if self.verbose:\n",
        "            print(f'Embedding shape: {embedding_out.shape}')\n",
        "        \n",
        "        lstm_out, _ = self.lstm(embedding_out)\n",
        "        if self.verbose:\n",
        "            print(f'LSTM output shape: {lstm_out.shape}')\n",
        "        \n",
        "        linear_out = self.linear(lstm_out)\n",
        "        if self.verbose:\n",
        "            print(f'Prediction shape: {linear_out.shape}')\n",
        "        \n",
        "        return linear_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jbrxsZ2mehWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "e75acb9c-9f19-4677-ead3-767f0e8f712b"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)  \n",
        "cur_correct_count, cur_sum_count = float(torch.sum(labels == y_batch)), labels.numel()  #\n",
        "\n",
        "accuracy = cur_correct_count / cur_sum_count\n",
        "\n",
        "print(f'\\nPrediction shape: {logits.shape}')\n",
        "print(f'True labels shape: {y_batch.shape}')\n",
        "\n",
        "print(f'\\nAccuracy: {accuracy:.4f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Prediction shape: torch.Size([32, 4, 13])\n",
            "True labels shape: torch.Size([32, 4])\n",
            "\n",
            "Accuracy: 0.0703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GMUyUm1hgpe3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "876cbe44-b546-452a-fa3c-8a7b0fddc039"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "logits_loss = torch.transpose(torch.transpose(logits, 0, 1), 1, 2)\n",
        "y_batch_loss = torch.transpose(y_batch, 0, 1)\n",
        "loss = criterion(logits_loss, y_batch_loss)\n",
        "\n",
        "print(f'Loss: {loss:.4f}')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss: 2.5823\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FprPQ0gllo7b",
        "colab": {}
      },
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "                logits_loss = torch.transpose(torch.transpose(logits, 0, 1), 1, 2)\n",
        "                y_batch_loss = torch.transpose(y_batch, 0, 1)\n",
        "                loss = criterion(logits_loss, y_batch_loss)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                #indices = torch.argmax(logits, -1)\n",
        "                #mask = y_batch.bool().float()\n",
        "                #match = (indices == y_batch).float()\n",
        "           \n",
        "                #cur_correct_count = torch.sum(match * mask)\n",
        "                #cur_sum_count = torch.sum(mask) \n",
        "\n",
        "                labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)  # my\n",
        "                cur_correct_count, cur_sum_count = float(torch.sum(labels == y_batch)), labels.numel()  # my\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pqfbeh1ltEYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a52b25ac-ab76-4ecc-890a-c4701036baf3"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.31355, Accuracy = 91.17%: 100%|██████████| 572/572 [00:05<00:00, 103.24it/s]\n",
            "[1 / 50]   Val: Loss = 0.10753, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 96.70it/s]\n",
            "[2 / 50] Train: Loss = 0.10096, Accuracy = 96.84%: 100%|██████████| 572/572 [00:05<00:00, 103.32it/s]\n",
            "[2 / 50]   Val: Loss = 0.07552, Accuracy = 97.95%: 100%|██████████| 13/13 [00:00<00:00, 95.56it/s]\n",
            "[3 / 50] Train: Loss = 0.06807, Accuracy = 97.85%: 100%|██████████| 572/572 [00:05<00:00, 103.75it/s]\n",
            "[3 / 50]   Val: Loss = 0.06918, Accuracy = 98.27%: 100%|██████████| 13/13 [00:00<00:00, 92.03it/s]\n",
            "[4 / 50] Train: Loss = 0.05103, Accuracy = 98.38%: 100%|██████████| 572/572 [00:05<00:00, 104.29it/s]\n",
            "[4 / 50]   Val: Loss = 0.06493, Accuracy = 98.45%: 100%|██████████| 13/13 [00:00<00:00, 96.70it/s]\n",
            "[5 / 50] Train: Loss = 0.04105, Accuracy = 98.69%: 100%|██████████| 572/572 [00:05<00:00, 104.88it/s]\n",
            "[5 / 50]   Val: Loss = 0.06504, Accuracy = 98.54%: 100%|██████████| 13/13 [00:00<00:00, 98.91it/s]\n",
            "[6 / 50] Train: Loss = 0.03348, Accuracy = 98.92%: 100%|██████████| 572/572 [00:05<00:00, 104.06it/s]\n",
            "[6 / 50]   Val: Loss = 0.06389, Accuracy = 98.64%: 100%|██████████| 13/13 [00:00<00:00, 96.81it/s]\n",
            "[7 / 50] Train: Loss = 0.02785, Accuracy = 99.10%: 100%|██████████| 572/572 [00:05<00:00, 104.58it/s]\n",
            "[7 / 50]   Val: Loss = 0.06503, Accuracy = 98.64%: 100%|██████████| 13/13 [00:00<00:00, 96.91it/s]\n",
            "[8 / 50] Train: Loss = 0.02302, Accuracy = 99.25%: 100%|██████████| 572/572 [00:05<00:00, 105.05it/s]\n",
            "[8 / 50]   Val: Loss = 0.06642, Accuracy = 98.71%: 100%|██████████| 13/13 [00:00<00:00, 95.37it/s]\n",
            "[9 / 50] Train: Loss = 0.01914, Accuracy = 99.38%: 100%|██████████| 572/572 [00:05<00:00, 104.58it/s]\n",
            "[9 / 50]   Val: Loss = 0.07576, Accuracy = 98.63%: 100%|██████████| 13/13 [00:00<00:00, 98.76it/s]\n",
            "[10 / 50] Train: Loss = 0.01598, Accuracy = 99.48%: 100%|██████████| 572/572 [00:05<00:00, 105.15it/s]\n",
            "[10 / 50]   Val: Loss = 0.07856, Accuracy = 98.55%: 100%|██████████| 13/13 [00:00<00:00, 103.89it/s]\n",
            "[11 / 50] Train: Loss = 0.01321, Accuracy = 99.57%: 100%|██████████| 572/572 [00:05<00:00, 105.27it/s]\n",
            "[11 / 50]   Val: Loss = 0.07275, Accuracy = 98.74%: 100%|██████████| 13/13 [00:00<00:00, 93.66it/s]\n",
            "[12 / 50] Train: Loss = 0.01094, Accuracy = 99.65%: 100%|██████████| 572/572 [00:05<00:00, 105.88it/s]\n",
            "[12 / 50]   Val: Loss = 0.08256, Accuracy = 98.67%: 100%|██████████| 13/13 [00:00<00:00, 93.06it/s]\n",
            "[13 / 50] Train: Loss = 0.00876, Accuracy = 99.73%: 100%|██████████| 572/572 [00:05<00:00, 105.74it/s]\n",
            "[13 / 50]   Val: Loss = 0.07946, Accuracy = 98.73%: 100%|██████████| 13/13 [00:00<00:00, 94.55it/s]\n",
            "[14 / 50] Train: Loss = 0.00719, Accuracy = 99.78%: 100%|██████████| 572/572 [00:05<00:00, 104.59it/s]\n",
            "[14 / 50]   Val: Loss = 0.08263, Accuracy = 98.73%: 100%|██████████| 13/13 [00:00<00:00, 90.58it/s]\n",
            "[15 / 50] Train: Loss = 0.00584, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 105.13it/s]\n",
            "[15 / 50]   Val: Loss = 0.09082, Accuracy = 98.66%: 100%|██████████| 13/13 [00:00<00:00, 96.50it/s]\n",
            "[16 / 50] Train: Loss = 0.00463, Accuracy = 99.87%: 100%|██████████| 572/572 [00:05<00:00, 104.61it/s]\n",
            "[16 / 50]   Val: Loss = 0.09349, Accuracy = 98.69%: 100%|██████████| 13/13 [00:00<00:00, 94.83it/s]\n",
            "[17 / 50] Train: Loss = 0.00384, Accuracy = 99.89%: 100%|██████████| 572/572 [00:05<00:00, 105.31it/s]\n",
            "[17 / 50]   Val: Loss = 0.09818, Accuracy = 98.67%: 100%|██████████| 13/13 [00:00<00:00, 97.61it/s]\n",
            "[18 / 50] Train: Loss = 0.00332, Accuracy = 99.91%: 100%|██████████| 572/572 [00:05<00:00, 105.36it/s]\n",
            "[18 / 50]   Val: Loss = 0.10975, Accuracy = 98.56%: 100%|██████████| 13/13 [00:00<00:00, 101.25it/s]\n",
            "[19 / 50] Train: Loss = 0.00272, Accuracy = 99.92%: 100%|██████████| 572/572 [00:05<00:00, 104.85it/s]\n",
            "[19 / 50]   Val: Loss = 0.10597, Accuracy = 98.65%: 100%|██████████| 13/13 [00:00<00:00, 96.47it/s]\n",
            "[20 / 50] Train: Loss = 0.00235, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 103.60it/s]\n",
            "[20 / 50]   Val: Loss = 0.11281, Accuracy = 98.59%: 100%|██████████| 13/13 [00:00<00:00, 97.83it/s]\n",
            "[21 / 50] Train: Loss = 0.00227, Accuracy = 99.93%: 100%|██████████| 572/572 [00:05<00:00, 103.64it/s]\n",
            "[21 / 50]   Val: Loss = 0.11152, Accuracy = 98.66%: 100%|██████████| 13/13 [00:00<00:00, 96.12it/s]\n",
            "[22 / 50] Train: Loss = 0.00199, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 105.03it/s]\n",
            "[22 / 50]   Val: Loss = 0.12348, Accuracy = 98.54%: 100%|██████████| 13/13 [00:00<00:00, 100.30it/s]\n",
            "[23 / 50] Train: Loss = 0.00196, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 105.64it/s]\n",
            "[23 / 50]   Val: Loss = 0.11985, Accuracy = 98.63%: 100%|██████████| 13/13 [00:00<00:00, 93.59it/s]\n",
            "[24 / 50] Train: Loss = 0.00202, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 105.91it/s]\n",
            "[24 / 50]   Val: Loss = 0.13428, Accuracy = 98.57%: 100%|██████████| 13/13 [00:00<00:00, 100.68it/s]\n",
            "[25 / 50] Train: Loss = 0.00191, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 105.06it/s]\n",
            "[25 / 50]   Val: Loss = 0.12292, Accuracy = 98.66%: 100%|██████████| 13/13 [00:00<00:00, 93.45it/s]\n",
            "[26 / 50] Train: Loss = 0.00167, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 104.79it/s]\n",
            "[26 / 50]   Val: Loss = 0.13158, Accuracy = 98.66%: 100%|██████████| 13/13 [00:00<00:00, 96.06it/s]\n",
            "[27 / 50] Train: Loss = 0.00167, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.51it/s]\n",
            "[27 / 50]   Val: Loss = 0.13721, Accuracy = 98.59%: 100%|██████████| 13/13 [00:00<00:00, 97.70it/s]\n",
            "[28 / 50] Train: Loss = 0.00163, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.63it/s]\n",
            "[28 / 50]   Val: Loss = 0.13295, Accuracy = 98.68%: 100%|██████████| 13/13 [00:00<00:00, 97.28it/s]\n",
            "[29 / 50] Train: Loss = 0.00156, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 104.43it/s]\n",
            "[29 / 50]   Val: Loss = 0.14039, Accuracy = 98.61%: 100%|██████████| 13/13 [00:00<00:00, 97.73it/s]\n",
            "[30 / 50] Train: Loss = 0.00172, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 103.79it/s]\n",
            "[30 / 50]   Val: Loss = 0.14219, Accuracy = 98.61%: 100%|██████████| 13/13 [00:00<00:00, 96.81it/s]\n",
            "[31 / 50] Train: Loss = 0.00176, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 104.29it/s]\n",
            "[31 / 50]   Val: Loss = 0.14427, Accuracy = 98.64%: 100%|██████████| 13/13 [00:00<00:00, 98.26it/s]\n",
            "[32 / 50] Train: Loss = 0.00156, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.97it/s]\n",
            "[32 / 50]   Val: Loss = 0.14023, Accuracy = 98.67%: 100%|██████████| 13/13 [00:00<00:00, 95.46it/s]\n",
            "[33 / 50] Train: Loss = 0.00146, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.35it/s]\n",
            "[33 / 50]   Val: Loss = 0.14818, Accuracy = 98.63%: 100%|██████████| 13/13 [00:00<00:00, 97.33it/s]\n",
            "[34 / 50] Train: Loss = 0.00141, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 104.91it/s]\n",
            "[34 / 50]   Val: Loss = 0.15497, Accuracy = 98.61%: 100%|██████████| 13/13 [00:00<00:00, 100.26it/s]\n",
            "[35 / 50] Train: Loss = 0.00146, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.36it/s]\n",
            "[35 / 50]   Val: Loss = 0.14122, Accuracy = 98.70%: 100%|██████████| 13/13 [00:00<00:00, 91.03it/s]\n",
            "[36 / 50] Train: Loss = 0.00140, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 106.27it/s]\n",
            "[36 / 50]   Val: Loss = 0.16337, Accuracy = 98.60%: 100%|██████████| 13/13 [00:00<00:00, 96.61it/s]\n",
            "[37 / 50] Train: Loss = 0.00215, Accuracy = 99.92%: 100%|██████████| 572/572 [00:05<00:00, 106.28it/s]\n",
            "[37 / 50]   Val: Loss = 0.15869, Accuracy = 98.59%: 100%|██████████| 13/13 [00:00<00:00, 98.27it/s]\n",
            "[38 / 50] Train: Loss = 0.00156, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 105.20it/s]\n",
            "[38 / 50]   Val: Loss = 0.14659, Accuracy = 98.71%: 100%|██████████| 13/13 [00:00<00:00, 93.16it/s]\n",
            "[39 / 50] Train: Loss = 0.00131, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.86it/s]\n",
            "[39 / 50]   Val: Loss = 0.17408, Accuracy = 98.55%: 100%|██████████| 13/13 [00:00<00:00, 101.39it/s]\n",
            "[40 / 50] Train: Loss = 0.00128, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.31it/s]\n",
            "[40 / 50]   Val: Loss = 0.16408, Accuracy = 98.65%: 100%|██████████| 13/13 [00:00<00:00, 88.68it/s]\n",
            "[41 / 50] Train: Loss = 0.00128, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 104.90it/s]\n",
            "[41 / 50]   Val: Loss = 0.16084, Accuracy = 98.67%: 100%|██████████| 13/13 [00:00<00:00, 97.02it/s]\n",
            "[42 / 50] Train: Loss = 0.00131, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.84it/s]\n",
            "[42 / 50]   Val: Loss = 0.17907, Accuracy = 98.53%: 100%|██████████| 13/13 [00:00<00:00, 99.07it/s]\n",
            "[43 / 50] Train: Loss = 0.00131, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.91it/s]\n",
            "[43 / 50]   Val: Loss = 0.17027, Accuracy = 98.62%: 100%|██████████| 13/13 [00:00<00:00, 100.90it/s]\n",
            "[44 / 50] Train: Loss = 0.00132, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.53it/s]\n",
            "[44 / 50]   Val: Loss = 0.16702, Accuracy = 98.61%: 100%|██████████| 13/13 [00:00<00:00, 92.81it/s]\n",
            "[45 / 50] Train: Loss = 0.00258, Accuracy = 99.91%: 100%|██████████| 572/572 [00:05<00:00, 105.10it/s]\n",
            "[45 / 50]   Val: Loss = 0.16887, Accuracy = 98.66%: 100%|██████████| 13/13 [00:00<00:00, 93.74it/s]\n",
            "[46 / 50] Train: Loss = 0.00139, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.45it/s]\n",
            "[46 / 50]   Val: Loss = 0.17435, Accuracy = 98.61%: 100%|██████████| 13/13 [00:00<00:00, 97.24it/s]\n",
            "[47 / 50] Train: Loss = 0.00124, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.67it/s]\n",
            "[47 / 50]   Val: Loss = 0.16923, Accuracy = 98.67%: 100%|██████████| 13/13 [00:00<00:00, 99.31it/s]\n",
            "[48 / 50] Train: Loss = 0.00120, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 104.24it/s]\n",
            "[48 / 50]   Val: Loss = 0.17643, Accuracy = 98.60%: 100%|██████████| 13/13 [00:00<00:00, 93.49it/s]\n",
            "[49 / 50] Train: Loss = 0.00122, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 104.32it/s]\n",
            "[49 / 50]   Val: Loss = 0.17490, Accuracy = 98.65%: 100%|██████████| 13/13 [00:00<00:00, 96.16it/s]\n",
            "[50 / 50] Train: Loss = 0.00121, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 105.32it/s]\n",
            "[50 / 50]   Val: Loss = 0.17995, Accuracy = 98.64%: 100%|██████████| 13/13 [00:00<00:00, 98.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBDOKz1aNh5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "                logits_loss = torch.transpose(torch.transpose(logits, 0, 1), 1, 2)\n",
        "                y_batch_loss = torch.transpose(y_batch, 0, 1)\n",
        "                loss = criterion(logits_loss, y_batch_loss)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                #indices = torch.argmax(logits, -1)\n",
        "                #mask = y_batch.bool().float()\n",
        "                #match = (indices == y_batch).float()\n",
        "           \n",
        "                #cur_correct_count = torch.sum(match * mask)\n",
        "                #cur_sum_count = torch.sum(mask) \n",
        "\n",
        "                mask = (y_batch != 0)\n",
        "                labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)  \n",
        "                cur_correct_count, cur_sum_count = float(torch.sum((labels == y_batch) * mask)), float(mask.sum())  \n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jfTkxfUNtaX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "83a6b9e1-51f9-4275-9408-ce6448b738cb"
      },
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 10] Train: Loss = 0.68711, Accuracy = 78.39%: 100%|██████████| 572/572 [00:05<00:00, 102.42it/s]\n",
            "[1 / 10]   Val: Loss = 0.35221, Accuracy = 88.43%: 100%|██████████| 13/13 [00:00<00:00, 95.19it/s]\n",
            "[2 / 10] Train: Loss = 0.27316, Accuracy = 90.94%: 100%|██████████| 572/572 [00:05<00:00, 103.93it/s]\n",
            "[2 / 10]   Val: Loss = 0.23584, Accuracy = 92.38%: 100%|██████████| 13/13 [00:00<00:00, 90.34it/s]\n",
            "[3 / 10] Train: Loss = 0.18397, Accuracy = 93.91%: 100%|██████████| 572/572 [00:05<00:00, 103.13it/s]\n",
            "[3 / 10]   Val: Loss = 0.19253, Accuracy = 93.87%: 100%|██████████| 13/13 [00:00<00:00, 92.78it/s]\n",
            "[4 / 10] Train: Loss = 0.13731, Accuracy = 95.42%: 100%|██████████| 572/572 [00:05<00:00, 103.26it/s]\n",
            "[4 / 10]   Val: Loss = 0.16903, Accuracy = 94.64%: 100%|██████████| 13/13 [00:00<00:00, 92.94it/s]\n",
            "[5 / 10] Train: Loss = 0.10662, Accuracy = 96.42%: 100%|██████████| 572/572 [00:05<00:00, 103.82it/s]\n",
            "[5 / 10]   Val: Loss = 0.15653, Accuracy = 95.00%: 100%|██████████| 13/13 [00:00<00:00, 97.31it/s]\n",
            "[6 / 10] Train: Loss = 0.08461, Accuracy = 97.15%: 100%|██████████| 572/572 [00:05<00:00, 103.91it/s]\n",
            "[6 / 10]   Val: Loss = 0.14856, Accuracy = 95.29%: 100%|██████████| 13/13 [00:00<00:00, 90.95it/s]\n",
            "[7 / 10] Train: Loss = 0.06838, Accuracy = 97.70%: 100%|██████████| 572/572 [00:05<00:00, 104.86it/s]\n",
            "[7 / 10]   Val: Loss = 0.15143, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 94.65it/s]\n",
            "[8 / 10] Train: Loss = 0.05528, Accuracy = 98.14%: 100%|██████████| 572/572 [00:05<00:00, 104.13it/s]\n",
            "[8 / 10]   Val: Loss = 0.15646, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 92.13it/s]\n",
            "[9 / 10] Train: Loss = 0.04542, Accuracy = 98.48%: 100%|██████████| 572/572 [00:05<00:00, 104.15it/s]\n",
            "[9 / 10]   Val: Loss = 0.15450, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 90.80it/s]\n",
            "[10 / 10] Train: Loss = 0.03729, Accuracy = 98.77%: 100%|██████████| 572/572 [00:05<00:00, 103.90it/s]\n",
            "[10 / 10]   Val: Loss = 0.16569, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 91.86it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "98wr38_rw55D",
        "colab": {}
      },
      "source": [
        "def compute_loss_accuracy(model, criterion, data, batch_size=512):    \n",
        "    model.eval() # Evaluation mode\n",
        "    \n",
        "    with torch.no_grad():    \n",
        "        loss_accum = 0.0\n",
        "        correct_count = 0.0\n",
        "        sum_count = 0.0\n",
        "        for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "            X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "            logits = model(X_batch)       \n",
        "            \n",
        "            logits_loss = torch.transpose(torch.transpose(logits, 0, 1), 1, 2)\n",
        "            y_batch_loss = torch.transpose(y_batch, 0, 1)\n",
        "            loss = criterion(logits_loss, y_batch_loss)\n",
        "            \n",
        "            loss_accum += loss.item()\n",
        "            \n",
        "            mask = (y_batch != 0)\n",
        "            labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)  \n",
        "            cur_correct_count, cur_sum_count = float(torch.sum((labels == y_batch) * mask)), float(mask.sum())  \n",
        "\n",
        "            correct_count += cur_correct_count\n",
        "            sum_count += cur_sum_count\n",
        "    \n",
        "    return loss_accum / (i + 1), correct_count / sum_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na-0uHaJOwKO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3ee1d8e2-8d97-48ff-8429-9309e1152ac6"
      },
      "source": [
        "test_loss, test_accuracy = compute_loss_accuracy(model, criterion, data=(X_test, y_test))\n",
        "print('Test: Loss = {:.5f}, Accuracy = {:.2%}'.format(test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test: Loss = 0.16105, Accuracy = 95.49%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMvTEJaENKBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1, verbose=False):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=word_emb_dim)\n",
        "        self.lstm = nn.LSTM(input_size=word_emb_dim, hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.linear = nn.Linear(in_features=2*lstm_hidden_dim, out_features=tagset_size)    \n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.verbose:\n",
        "            print(f'Input shape: {inputs.shape}')\n",
        "            \n",
        "        embedding_out = self.embedding(inputs)\n",
        "        if self.verbose:\n",
        "            print(f'Embedding shape: {embedding_out.shape}')\n",
        "        \n",
        "        lstm_out, _ = self.lstm(embedding_out)\n",
        "        if self.verbose:\n",
        "            print(f'LSTM output shape: {lstm_out.shape}')\n",
        "        \n",
        "        linear_out = self.linear(lstm_out)\n",
        "        if self.verbose:\n",
        "            print(f'Prediction shape: {linear_out.shape}')\n",
        "        \n",
        "        return linear_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3p8S_JbVywH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "e3225e1b-8043-4913-f71b-44d6c339e689"
      },
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind),\n",
        ").cuda()\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "labels = torch.argmax(logits.view(y_batch.size()[0], y_batch.size()[1], -1), dim=2)  \n",
        "cur_correct_count, cur_sum_count = float(torch.sum(labels == y_batch)), labels.numel()  #\n",
        "\n",
        "accuracy = cur_correct_count / cur_sum_count\n",
        "\n",
        "print(f'\\nPrediction shape: {logits.shape}')\n",
        "print(f'True labels shape: {y_batch.shape}')\n",
        "\n",
        "print(f'\\nAccuracy: {accuracy:.4f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-f306875254bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-5ab5939baa8c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Input shape: {inputs.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0membedding_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Embedding shape: {embedding_out.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m         return F.embedding(\n\u001b[1;32m    109\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type torch.cuda.LongTensor but found type torch.LongTensor for argument #3 'index'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inuO8_tXTiOC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "ba4d15f5-4ca8-4b5c-e44d-ee12fcc6003f"
      },
      "source": [
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind),\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=10,\n",
        "    batch_size=64, \n",
        "    val_data=(X_val, y_val), \n",
        "    val_batch_size=13)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 10] Train: Loss = 0.55692, Accuracy = 82.31%: 100%|██████████| 572/572 [00:06<00:00, 86.99it/s]\n",
            "[1 / 10]   Val: Loss = 0.27867, Accuracy = 91.11%: 100%|██████████| 497/497 [00:02<00:00, 233.24it/s]\n",
            "[2 / 10] Train: Loss = 0.20389, Accuracy = 93.53%: 100%|██████████| 572/572 [00:06<00:00, 86.53it/s]\n",
            "[2 / 10]   Val: Loss = 0.17986, Accuracy = 94.30%: 100%|██████████| 497/497 [00:02<00:00, 237.05it/s]\n",
            "[3 / 10] Train: Loss = 0.12894, Accuracy = 96.01%: 100%|██████████| 572/572 [00:06<00:00, 87.79it/s]\n",
            "[3 / 10]   Val: Loss = 0.14625, Accuracy = 95.39%: 100%|██████████| 497/497 [00:02<00:00, 232.41it/s]\n",
            "[4 / 10] Train: Loss = 0.08803, Accuracy = 97.31%: 100%|██████████| 572/572 [00:06<00:00, 88.22it/s]\n",
            "[4 / 10]   Val: Loss = 0.12852, Accuracy = 95.92%: 100%|██████████| 497/497 [00:02<00:00, 241.61it/s]\n",
            "[5 / 10] Train: Loss = 0.06073, Accuracy = 98.19%: 100%|██████████| 572/572 [00:06<00:00, 88.89it/s]\n",
            "[5 / 10]   Val: Loss = 0.12396, Accuracy = 96.18%: 100%|██████████| 497/497 [00:02<00:00, 242.83it/s]\n",
            "[6 / 10] Train: Loss = 0.04167, Accuracy = 98.80%: 100%|██████████| 572/572 [00:06<00:00, 89.22it/s]\n",
            "[6 / 10]   Val: Loss = 0.12756, Accuracy = 96.17%: 100%|██████████| 497/497 [00:02<00:00, 243.12it/s]\n",
            "[7 / 10] Train: Loss = 0.02788, Accuracy = 99.24%: 100%|██████████| 572/572 [00:06<00:00, 87.00it/s]\n",
            "[7 / 10]   Val: Loss = 0.12649, Accuracy = 96.35%: 100%|██████████| 497/497 [00:02<00:00, 242.78it/s]\n",
            "[8 / 10] Train: Loss = 0.01846, Accuracy = 99.53%: 100%|██████████| 572/572 [00:06<00:00, 89.14it/s]\n",
            "[8 / 10]   Val: Loss = 0.13481, Accuracy = 96.35%: 100%|██████████| 497/497 [00:02<00:00, 239.21it/s]\n",
            "[9 / 10] Train: Loss = 0.01201, Accuracy = 99.71%: 100%|██████████| 572/572 [00:06<00:00, 89.53it/s]\n",
            "[9 / 10]   Val: Loss = 0.13971, Accuracy = 96.39%: 100%|██████████| 497/497 [00:02<00:00, 244.30it/s]\n",
            "[10 / 10] Train: Loss = 0.00759, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 89.56it/s]\n",
            "[10 / 10]   Val: Loss = 0.15149, Accuracy = 96.42%: 100%|██████████| 497/497 [00:02<00:00, 238.39it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8KBqOjUYEeR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1525e7bd-9112-4c2a-8cc7-a05782ea6db1"
      },
      "source": [
        "test_loss, test_accuracy = compute_loss_accuracy(model, criterion, data=(X_test, y_test))\n",
        "print('Test: Loss = {:.5f}, Accuracy = {:.2%}'.format(test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test: Loss = 0.15225, Accuracy = 96.44%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THgtssy2ThVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uZpY_Q1xZ18h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "cd4bb625-83d9-4a09-bad2-89793f690c29"
      },
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VsCstxiO03oT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2224388f-ac6d-4148-d00a-34dd8eff5420"
      },
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzruVvBbeJZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "embeddings = torch.FloatTensor(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LxaRBpQd0pat",
        "colab": {}
      },
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embedding, tagset_size, lstm_hidden_dim=128, lstm_layers_count=1, verbose=False):\n",
        "        super().__init__()\n",
        "        self.verbose = verbose\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding, freeze = True)\n",
        "        self.lstm = nn.LSTM(input_size=w2v_model.vectors.shape[1], hidden_size=lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.linear = nn.Linear(in_features = 2*lstm_hidden_dim, out_features = tagset_size) \n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.verbose:\n",
        "            print(f'Input shape: {inputs.shape}')\n",
        "            \n",
        "        embedding_out = self.embedding(inputs)\n",
        "        if self.verbose:\n",
        "            print(f'Embedding shape: {embedding_out.shape}')\n",
        "        \n",
        "        lstm_out, _ = self.lstm(embedding_out)\n",
        "        if self.verbose:\n",
        "            print(f'LSTM output shape: {lstm_out.shape}')\n",
        "        \n",
        "        linear_out = self.linear(lstm_out)\n",
        "        if self.verbose:\n",
        "            print(f'Prediction shape: {linear_out.shape}')\n",
        "            \n",
        "        return linear_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7E36-UEeC8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EBtI6BDE-Fc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60068269-f48b-4ce9-ef10-8175bb184726"
      },
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embedding=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 / 50] Train: Loss = 0.47233, Accuracy = 86.04%: 100%|██████████| 572/572 [00:05<00:00, 110.49it/s]\n",
            "[1 / 50]   Val: Loss = 0.22335, Accuracy = 93.39%: 100%|██████████| 13/13 [00:00<00:00, 73.99it/s]\n",
            "[2 / 50] Train: Loss = 0.16304, Accuracy = 95.11%: 100%|██████████| 572/572 [00:05<00:00, 112.38it/s]\n",
            "[2 / 50]   Val: Loss = 0.15881, Accuracy = 95.09%: 100%|██████████| 13/13 [00:00<00:00, 79.89it/s]\n",
            "[3 / 50] Train: Loss = 0.11651, Accuracy = 96.48%: 100%|██████████| 572/572 [00:05<00:00, 111.98it/s]\n",
            "[3 / 50]   Val: Loss = 0.12654, Accuracy = 96.04%: 100%|██████████| 13/13 [00:00<00:00, 78.44it/s]\n",
            "[4 / 50] Train: Loss = 0.09310, Accuracy = 97.17%: 100%|██████████| 572/572 [00:05<00:00, 112.91it/s]\n",
            "[4 / 50]   Val: Loss = 0.11198, Accuracy = 96.52%: 100%|██████████| 13/13 [00:00<00:00, 79.92it/s]\n",
            "[5 / 50] Train: Loss = 0.07920, Accuracy = 97.59%: 100%|██████████| 572/572 [00:05<00:00, 110.87it/s]\n",
            "[5 / 50]   Val: Loss = 0.10512, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 77.73it/s]\n",
            "[6 / 50] Train: Loss = 0.06932, Accuracy = 97.87%: 100%|██████████| 572/572 [00:05<00:00, 111.93it/s]\n",
            "[6 / 50]   Val: Loss = 0.10097, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 77.43it/s]\n",
            "[7 / 50] Train: Loss = 0.06137, Accuracy = 98.10%: 100%|██████████| 572/572 [00:05<00:00, 109.60it/s]\n",
            "[7 / 50]   Val: Loss = 0.09604, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 72.95it/s]\n",
            "[8 / 50] Train: Loss = 0.05525, Accuracy = 98.27%: 100%|██████████| 572/572 [00:05<00:00, 109.26it/s]\n",
            "[8 / 50]   Val: Loss = 0.09478, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 76.83it/s]\n",
            "[9 / 50] Train: Loss = 0.04972, Accuracy = 98.46%: 100%|██████████| 572/572 [00:05<00:00, 111.26it/s]\n",
            "[9 / 50]   Val: Loss = 0.09474, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 78.89it/s]\n",
            "[10 / 50] Train: Loss = 0.04452, Accuracy = 98.63%: 100%|██████████| 572/572 [00:05<00:00, 112.19it/s]\n",
            "[10 / 50]   Val: Loss = 0.09325, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 76.10it/s]\n",
            "[11 / 50] Train: Loss = 0.04015, Accuracy = 98.76%: 100%|██████████| 572/572 [00:05<00:00, 110.87it/s]\n",
            "[11 / 50]   Val: Loss = 0.10184, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 77.50it/s]\n",
            "[12 / 50] Train: Loss = 0.03592, Accuracy = 98.91%: 100%|██████████| 572/572 [00:05<00:00, 112.20it/s]\n",
            "[12 / 50]   Val: Loss = 0.09550, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 74.03it/s]\n",
            "[13 / 50] Train: Loss = 0.03213, Accuracy = 99.03%: 100%|██████████| 572/572 [00:05<00:00, 111.11it/s]\n",
            "[13 / 50]   Val: Loss = 0.09680, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 77.13it/s]\n",
            "[14 / 50] Train: Loss = 0.02855, Accuracy = 99.15%: 100%|██████████| 572/572 [00:05<00:00, 112.00it/s]\n",
            "[14 / 50]   Val: Loss = 0.10230, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 79.33it/s]\n",
            "[15 / 50] Train: Loss = 0.02529, Accuracy = 99.26%: 100%|██████████| 572/572 [00:05<00:00, 111.69it/s]\n",
            "[15 / 50]   Val: Loss = 0.10503, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 76.97it/s]\n",
            "[16 / 50] Train: Loss = 0.02195, Accuracy = 99.38%: 100%|██████████| 572/572 [00:05<00:00, 112.97it/s]\n",
            "[16 / 50]   Val: Loss = 0.10717, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 77.00it/s]\n",
            "[17 / 50] Train: Loss = 0.01954, Accuracy = 99.45%: 100%|██████████| 572/572 [00:05<00:00, 112.93it/s]\n",
            "[17 / 50]   Val: Loss = 0.11018, Accuracy = 96.91%: 100%|██████████| 13/13 [00:00<00:00, 74.87it/s]\n",
            "[18 / 50] Train: Loss = 0.01688, Accuracy = 99.55%: 100%|██████████| 572/572 [00:05<00:00, 112.27it/s]\n",
            "[18 / 50]   Val: Loss = 0.11296, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 77.52it/s]\n",
            "[19 / 50] Train: Loss = 0.01431, Accuracy = 99.64%: 100%|██████████| 572/572 [00:05<00:00, 112.02it/s]\n",
            "[19 / 50]   Val: Loss = 0.11741, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 73.98it/s]\n",
            "[20 / 50] Train: Loss = 0.01237, Accuracy = 99.70%: 100%|██████████| 572/572 [00:05<00:00, 111.13it/s]\n",
            "[20 / 50]   Val: Loss = 0.12360, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 76.53it/s]\n",
            "[21 / 50] Train: Loss = 0.01026, Accuracy = 99.78%: 100%|██████████| 572/572 [00:05<00:00, 111.44it/s]\n",
            "[21 / 50]   Val: Loss = 0.13462, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 72.85it/s]\n",
            "[22 / 50] Train: Loss = 0.00891, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 110.90it/s]\n",
            "[22 / 50]   Val: Loss = 0.13374, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 75.27it/s]\n",
            "[23 / 50] Train: Loss = 0.00744, Accuracy = 99.86%: 100%|██████████| 572/572 [00:05<00:00, 110.34it/s]\n",
            "[23 / 50]   Val: Loss = 0.13810, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 73.76it/s]\n",
            "[24 / 50] Train: Loss = 0.00651, Accuracy = 99.88%: 100%|██████████| 572/572 [00:05<00:00, 111.62it/s]\n",
            "[24 / 50]   Val: Loss = 0.14464, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 77.20it/s]\n",
            "[25 / 50] Train: Loss = 0.00557, Accuracy = 99.90%: 100%|██████████| 572/572 [00:05<00:00, 112.35it/s]\n",
            "[25 / 50]   Val: Loss = 0.14737, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 77.15it/s]\n",
            "[26 / 50] Train: Loss = 0.00457, Accuracy = 99.93%: 100%|██████████| 572/572 [00:05<00:00, 113.26it/s]\n",
            "[26 / 50]   Val: Loss = 0.15142, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 77.72it/s]\n",
            "[27 / 50] Train: Loss = 0.00408, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 113.11it/s]\n",
            "[27 / 50]   Val: Loss = 0.15911, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 79.37it/s]\n",
            "[28 / 50] Train: Loss = 0.00362, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 112.12it/s]\n",
            "[28 / 50]   Val: Loss = 0.16474, Accuracy = 96.68%: 100%|██████████| 13/13 [00:00<00:00, 76.58it/s]\n",
            "[29 / 50] Train: Loss = 0.00320, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 111.54it/s]\n",
            "[29 / 50]   Val: Loss = 0.16595, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 73.75it/s]\n",
            "[30 / 50] Train: Loss = 0.00321, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 113.06it/s]\n",
            "[30 / 50]   Val: Loss = 0.17709, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 72.24it/s]\n",
            "[31 / 50] Train: Loss = 0.00342, Accuracy = 99.94%: 100%|██████████| 572/572 [00:05<00:00, 112.33it/s]\n",
            "[31 / 50]   Val: Loss = 0.17582, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 76.90it/s]\n",
            "[32 / 50] Train: Loss = 0.00207, Accuracy = 99.98%: 100%|██████████| 572/572 [00:05<00:00, 113.30it/s]\n",
            "[32 / 50]   Val: Loss = 0.17982, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 76.31it/s]\n",
            "[33 / 50] Train: Loss = 0.00151, Accuracy = 99.99%: 100%|██████████| 572/572 [00:05<00:00, 114.06it/s]\n",
            "[33 / 50]   Val: Loss = 0.18571, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 80.77it/s]\n",
            "[34 / 50] Train: Loss = 0.00422, Accuracy = 99.88%: 100%|██████████| 572/572 [00:05<00:00, 108.77it/s]\n",
            "[34 / 50]   Val: Loss = 0.20219, Accuracy = 96.55%: 100%|██████████| 13/13 [00:00<00:00, 76.22it/s]\n",
            "[35 / 50] Train: Loss = 0.00426, Accuracy = 99.89%: 100%|██████████| 572/572 [00:05<00:00, 108.35it/s]\n",
            "[35 / 50]   Val: Loss = 0.18553, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 76.08it/s]\n",
            "[36 / 50] Train: Loss = 0.00113, Accuracy = 99.99%: 100%|██████████| 572/572 [00:05<00:00, 109.15it/s]\n",
            "[36 / 50]   Val: Loss = 0.18883, Accuracy = 96.75%: 100%|██████████| 13/13 [00:00<00:00, 75.65it/s]\n",
            "[37 / 50] Train: Loss = 0.00069, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 109.98it/s]\n",
            "[37 / 50]   Val: Loss = 0.19511, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 76.55it/s]\n",
            "[38 / 50] Train: Loss = 0.00054, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 112.36it/s]\n",
            "[38 / 50]   Val: Loss = 0.19416, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 77.00it/s]\n",
            "[39 / 50] Train: Loss = 0.00559, Accuracy = 99.82%: 100%|██████████| 572/572 [00:05<00:00, 112.79it/s]\n",
            "[39 / 50]   Val: Loss = 0.19604, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 76.65it/s]\n",
            "[40 / 50] Train: Loss = 0.00399, Accuracy = 99.89%: 100%|██████████| 572/572 [00:05<00:00, 114.30it/s]\n",
            "[40 / 50]   Val: Loss = 0.18916, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 75.04it/s]\n",
            "[41 / 50] Train: Loss = 0.00096, Accuracy = 99.99%: 100%|██████████| 572/572 [00:05<00:00, 112.80it/s]\n",
            "[41 / 50]   Val: Loss = 0.19409, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 78.48it/s]\n",
            "[42 / 50] Train: Loss = 0.00055, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 113.27it/s]\n",
            "[42 / 50]   Val: Loss = 0.20130, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 77.40it/s]\n",
            "[43 / 50] Train: Loss = 0.00046, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 112.81it/s]\n",
            "[43 / 50]   Val: Loss = 0.19856, Accuracy = 96.77%: 100%|██████████| 13/13 [00:00<00:00, 78.28it/s]\n",
            "[44 / 50] Train: Loss = 0.00056, Accuracy = 99.99%: 100%|██████████| 572/572 [00:05<00:00, 111.51it/s]\n",
            "[44 / 50]   Val: Loss = 0.20618, Accuracy = 96.71%: 100%|██████████| 13/13 [00:00<00:00, 74.60it/s]\n",
            "[45 / 50] Train: Loss = 0.00772, Accuracy = 99.74%: 100%|██████████| 572/572 [00:05<00:00, 112.59it/s]\n",
            "[45 / 50]   Val: Loss = 0.20550, Accuracy = 96.62%: 100%|██████████| 13/13 [00:00<00:00, 78.94it/s]\n",
            "[46 / 50] Train: Loss = 0.00228, Accuracy = 99.95%: 100%|██████████| 572/572 [00:05<00:00, 113.10it/s]\n",
            "[46 / 50]   Val: Loss = 0.20245, Accuracy = 96.70%: 100%|██████████| 13/13 [00:00<00:00, 79.20it/s]\n",
            "[47 / 50] Train: Loss = 0.00071, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 113.93it/s]\n",
            "[47 / 50]   Val: Loss = 0.20428, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 76.37it/s]\n",
            "[48 / 50] Train: Loss = 0.00045, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 114.25it/s]\n",
            "[48 / 50]   Val: Loss = 0.20682, Accuracy = 96.80%: 100%|██████████| 13/13 [00:00<00:00, 75.75it/s]\n",
            "[49 / 50] Train: Loss = 0.00036, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 114.23it/s]\n",
            "[49 / 50]   Val: Loss = 0.21299, Accuracy = 96.76%: 100%|██████████| 13/13 [00:00<00:00, 74.91it/s]\n",
            "[50 / 50] Train: Loss = 0.00047, Accuracy = 100.00%: 100%|██████████| 572/572 [00:05<00:00, 113.00it/s]\n",
            "[50 / 50]   Val: Loss = 0.21302, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 74.28it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPUuAPGhEGVR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e48f1629-6fb7-46cd-a591-d6e0d1c30b9b"
      },
      "source": [
        "test_loss, test_accuracy = compute_loss_accuracy(model, criterion, data=(X_test, y_test))\n",
        "print('Test: Loss = {:.5f}, Accuracy = {:.2%}'.format(test_loss, test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test: Loss = 0.21331, Accuracy = 96.82%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}